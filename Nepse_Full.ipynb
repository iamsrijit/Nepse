{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ex1LOmfXjZ3"
      },
      "outputs": [],
      "source": [
        "!pip install nepse-scraper\n",
        "!pip install xlsxwriter\n",
        "!pip install gitpython\n",
        "!pip install gitpython pandas\n",
        "from nepse_scraper import Nepse_scraper\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create an object from the Nepse_scraper class\n",
        "request_obj = Nepse_scraper()\n",
        "\n",
        "# Get today's price from NEPSE\n",
        "today_price = request_obj.get_today_price()\n",
        "\n",
        "# Extract the 'content' section from the response\n",
        "content_data = today_price.get('content', [])\n",
        "\n",
        "# Initialize an empty list to store filtered data\n",
        "filtered_data = []\n",
        "\n",
        "# Define the column names\n",
        "columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close','Volume']\n",
        "\n",
        "# Iterate over each item in the 'content' section\n",
        "for item in content_data:\n",
        "    symbol = item.get('symbol', '')\n",
        "    date = item.get('businessDate', '')\n",
        "    open_price = item.get('openPrice', '')\n",
        "    high_price = item.get('highPrice', '')\n",
        "    low_price = item.get('lowPrice', '')\n",
        "    close_price = item.get('closePrice', '')\n",
        "    Volume_daily = item.get('totalTradedValue', '')\n",
        "\n",
        "    # Append the extracted values to the filtered data list\n",
        "    filtered_data.append({\n",
        "        'Symbol': symbol,\n",
        "        'Date': date,\n",
        "        'Open': open_price,\n",
        "        'High': high_price,\n",
        "        'Low': low_price,\n",
        "        'Close': close_price,\n",
        "        'Volume': Volume_daily\n",
        "    })\n",
        "\n",
        "# Create DataFrame from filtered data\n",
        "first = pd.DataFrame(filtered_data)\n",
        "\n",
        "# Check if DataFrame has data\n",
        "if not first.empty:\n",
        "    # Display DataFrame\n",
        "    print(first)\n",
        "\n",
        "    # Get today's day name\n",
        "    today_day_name = datetime.now().strftime('%A')\n",
        "\n",
        "    # Save DataFrame to CSV with today's day name in the filename\n",
        "    file_name = f'nepse_{today_day_name}.csv'\n",
        "    first.to_csv(file_name, index=False)\n",
        "\n",
        "    print(f\"Data saved to '{file_name}'\")\n",
        "else:\n",
        "    print(\"No data available to create DataFrame.\")\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to get the latest file URL\n",
        "def get_latest_file_url(repo_url):\n",
        "    # Send a GET request to the GitHub repository\n",
        "    response = requests.get(repo_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all links to files in the repository\n",
        "    file_links = soup.find_all('a', href=True)\n",
        "\n",
        "    # Extract file names and corresponding URLs\n",
        "    file_urls = {}\n",
        "    for link in file_links:\n",
        "        file_name = link['href']\n",
        "        if file_name.endswith('.csv'):\n",
        "            date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', file_name)\n",
        "            if date_match:\n",
        "                file_date = date_match.group(1)\n",
        "                file_urls[file_date] = repo_url.replace('/tree/', '/raw/') + '/' + file_name\n",
        "\n",
        "    if not file_urls:\n",
        "        raise ValueError(\"No CSV files found in the repository.\")\n",
        "\n",
        "    # Get the latest file URL based on the date\n",
        "    latest_file_date = max(file_urls.keys())\n",
        "    latest_file_url = file_urls[latest_file_date]\n",
        "    print(\"Latest file date:\", latest_file_date)\n",
        "    print(\"Latest file URL:\", latest_file_url)\n",
        "    return latest_file_url\n",
        "\n",
        "# Replace with the actual GitHub repository URL\n",
        "repo_url = 'https://github.com/iamsrijit/Nepse/tree/main'\n",
        "\n",
        "try:\n",
        "    # Get the latest file URL\n",
        "    latest_file_url = get_latest_file_url(repo_url)\n",
        "\n",
        "    # Correct the file URL\n",
        "    latest_file_url = latest_file_url.replace('/iamsrijit/Nepse/blob/main/', '/')\n",
        "\n",
        "    # Read data from the latest file\n",
        "    secondss = pd.read_csv(latest_file_url)\n",
        "\n",
        "    # Assuming the column names are the same as in your example\n",
        "    secondss.columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close','Volume']\n",
        "\n",
        "    print(secondss.head())\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'first' and 'secondss' are your DataFrames\n",
        "dfs = [first, secondss]\n",
        "\n",
        "# Create an empty DataFrame to store the final results\n",
        "final_df = pd.DataFrame()\n",
        "\n",
        "for df in dfs:\n",
        "    try:\n",
        "        if 'Date' not in df.columns:\n",
        "            print(f\"'Date' column not found in DataFrame.\")\n",
        "            continue\n",
        "\n",
        "        # Convert the 'Date' column to datetime format and drop rows with invalid dates\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%m/%d/%Y')\n",
        "        df.dropna(subset=['Date'], inplace=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing DataFrame: {e}\")\n",
        "\n",
        "# Combine all the DataFrames\n",
        "if not dfs:\n",
        "    print(\"No valid data to process.\")\n",
        "else:\n",
        "    combined_df = pd.concat(dfs, ignore_index=True, join='outer')\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], format='%m/%d/%Y')\n",
        "\n",
        "    # Iterate over each unique symbol\n",
        "    for symbol in combined_df['Symbol'].unique():\n",
        "        symbol_df = combined_df[combined_df['Symbol'] == symbol]\n",
        "        symbol_df = symbol_df.sort_values(by=['Date'], ascending=False)\n",
        "        symbol_df = symbol_df.drop_duplicates(subset=['Date'], keep='first')\n",
        "\n",
        "        # Initialize columns 'G' and 'H' as None\n",
        "        symbol_df['G'] = None\n",
        "        symbol_df['H'] = None\n",
        "\n",
        "        # Append to the final DataFrame\n",
        "        final_df = pd.concat([final_df, symbol_df], ignore_index=True)\n",
        "\n",
        "    # Save the final DataFrame to a CSV file\n",
        "    output_file_name = 'combined_data.csv'\n",
        "\n",
        "    # Write the headers to the CSV file\n",
        "    with open(output_file_name, 'w') as f:\n",
        "        headers = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'G', 'H','Volume']\n",
        "        f.write(','.join(headers) + '\\n')\n",
        "\n",
        "    # Append the final DataFrame without headers to the CSV file\n",
        "    final_df.to_csv(output_file_name, mode='a', index=False, header=False)\n",
        "\n",
        "    # Optional: print the first few rows of the final DataFrame\n",
        "    # print(final_df.head())\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'first' and 'secondss' are your DataFrames\n",
        "dfs = [first, secondss]\n",
        "\n",
        "# Create an empty DataFrame to store the final results\n",
        "final_df = pd.DataFrame()\n",
        "\n",
        "# List of symbols to exclude\n",
        "exclude_symbols = [\n",
        "    'SAEF', 'SEF', 'CMF1', 'NICGF', 'NBF2', 'CMF2', 'NMB50', 'SIGS2', 'NICBF',\n",
        "    'SFMF', 'LUK', 'SLCF', 'KEF', 'SBCF', 'PSF', 'NIBSF2', 'NICSF', 'RMF1',\n",
        "    'NBF3', 'MMF1', 'KDBY', 'NICFC', 'GIBF1', 'NSIF2', 'SAGF', 'NIBLGF',\n",
        "    'SFEF', 'PRSF', 'C30MF', 'SIGS3', 'RMF2', 'LVF2', 'H8020', 'NICGF2',\n",
        "    'NIBLSTF', 'KSY' ,'NBLD87', 'PBD88', 'OTHERS','HIDCLP','NIMBPO','MUTUAL',\n",
        "    'CIT','ILI','LEMF','NIBLPF','INVESTMENT','SENFLOAT','HEIP' ,'SBID83','NICAD8283'\n",
        "]\n",
        "\n",
        "for df in dfs:\n",
        "    try:\n",
        "        if 'Date' not in df.columns:\n",
        "            print(f\"'Date' column not found in DataFrame.\")\n",
        "            continue\n",
        "\n",
        "        # Convert the 'Date' column to datetime format and drop rows with invalid dates\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%m/%d/%Y')\n",
        "        df.dropna(subset=['Date'], inplace=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing DataFrame: {e}\")\n",
        "\n",
        "# Combine all the DataFrames\n",
        "if not dfs:\n",
        "    print(\"No valid data to process.\")\n",
        "else:\n",
        "    combined_df = pd.concat(dfs, ignore_index=True, join='outer')\n",
        "\n",
        "    # Filter out the excluded symbols\n",
        "    combined_df = combined_df[~combined_df['Symbol'].isin(exclude_symbols)]\n",
        "\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], format='%m/%d/%Y')\n",
        "\n",
        "    # Iterate over each unique symbol\n",
        "    for symbol in combined_df['Symbol'].unique():\n",
        "        symbol_df = combined_df[combined_df['Symbol'] == symbol]\n",
        "        symbol_df = symbol_df.sort_values(by=['Date'], ascending=False)\n",
        "        symbol_df = symbol_df.drop_duplicates(subset=['Date'], keep='first')\n",
        "\n",
        "        # Initialize columns 'G' and 'H' as None\n",
        "        symbol_df['G'] = None\n",
        "        symbol_df['H'] = None\n",
        "\n",
        "        # Append to the final DataFrame\n",
        "        final_df = pd.concat([final_df, symbol_df], ignore_index=True)\n",
        "\n",
        "    # Save the final DataFrame to a CSV file\n",
        "    output_file_name = 'combined_data.csv'\n",
        "\n",
        "    # Write the headers to the CSV file\n",
        "    with open(output_file_name, 'w') as f:\n",
        "        headers = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'G', 'H','Volume']\n",
        "        f.write(','.join(headers) + '\\n')\n",
        "\n",
        "    # Append the final DataFrame without headers to the CSV file\n",
        "    final_df.to_csv(output_file_name, mode='a', index=False, header=False)\n",
        "\n",
        "    # Optional: print the first few rows of the final DataFrame\n",
        "    # print(final_df.head())\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import base64\n",
        "\n",
        "# Assuming you have sorted_df DataFrame containing your data\n",
        "# Assuming sorted_df is defined elsewhere in your code\n",
        "\n",
        "try:\n",
        "    # Convert sorted_df to CSV format\n",
        "    csv_data = final_df.to_csv(index=False)\n",
        "\n",
        "    # Encode the CSV data to Base64\n",
        "    csv_data_base64 = base64.b64encode(csv_data.encode()).decode()\n",
        "\n",
        "    # Define the GitHub repository URL\n",
        "    repo_url = 'https://github.com/iamsrijit/Nepse'\n",
        "\n",
        "    # Define the file name with today's date\n",
        "    file_name = f'espen_{datetime.today().strftime(\"%Y-%m-%d\")}.csv'\n",
        "\n",
        "    # Define your personal access token\n",
        "    token = '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'\n",
        "\n",
        "    # Define the file path in the repository\n",
        "    file_path = f'/{file_name}'\n",
        "\n",
        "    # Define the API URL for uploading files to GitHub\n",
        "    upload_url = f'https://api.github.com/repos/iamsrijit/Nepse/contents{file_path}'\n",
        "\n",
        "    # Prepare the headers with the authorization token\n",
        "    headers = {\n",
        "        'Authorization': f'token {token}',\n",
        "        'Accept': 'application/vnd.github.v3+json'\n",
        "    }\n",
        "\n",
        "    # Prepare the payload with file content\n",
        "    payload = {\n",
        "        'message': f'Upload {file_name}',\n",
        "        'content': csv_data_base64,\n",
        "        'branch': 'main'  # Specify the branch you want to upload to\n",
        "    }\n",
        "\n",
        "    # Send a PUT request to upload the file\n",
        "    response = requests.put(upload_url, headers=headers, json=payload)\n",
        "\n",
        "    # Check the response status\n",
        "    if response.status_code == 200:\n",
        "        print(f'File {file_name} uploaded successfully!')\n",
        "    elif response.status_code == 422:\n",
        "        print(f'Failed to upload {file_name}. Status code: 422 Unprocessable Entity')\n",
        "        print('Error Message:', response.json()['message'])\n",
        "    else:\n",
        "        print(f'Failed to upload {file_name}. Status code: {response.status_code}')\n",
        "        print('Response Content:', response.text)\n",
        "\n",
        "except Exception as e:\n",
        "    print('An error occurred:', e)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Sample DataFrame (final_df) should have columns ['Symbol', 'Date', 'Close']\n",
        "# Make sure final_df is sorted by 'Symbol' and 'Date' (in ascending order)\n",
        "final_df = final_df.sort_values(by=['Symbol', 'Date'], ascending=[True, True])\n",
        "\n",
        "# Check if the number of unique symbols is less than 50\n",
        "if final_df['Symbol'].nunique() < 50:\n",
        "    print(\"The number of unique symbols is less than 50. Please provide more data.\")\n",
        "else:\n",
        "    # Initialize an empty list to store results for each symbol\n",
        "    results = []\n",
        "    insufficient_data = []\n",
        "\n",
        "    # Function to calculate EMA\n",
        "    def calculate_ema(prices, span):\n",
        "        ema = np.full(len(prices), np.nan)\n",
        "        if len(prices) < span:\n",
        "            return ema\n",
        "        ema[span-1] = sum(prices[:span]) / span\n",
        "        multiplier = 2 / (span + 1)\n",
        "        for i in range(span, len(prices)):\n",
        "            ema[i] = (prices[i] - ema[i-1]) * multiplier + ema[i-1]\n",
        "        return ema\n",
        "\n",
        "    # Function to calculate RSI\n",
        "    def calculate_rsi(prices, period=14):\n",
        "        deltas = np.diff(prices)\n",
        "        seed = deltas[:period+1]\n",
        "        up = seed[seed >= 0].sum()/period\n",
        "        down = -seed[seed < 0].sum()/period\n",
        "        rs = up/down\n",
        "        rsi = np.zeros_like(prices)\n",
        "        rsi[:period] = 100. - 100./(1. + rs)\n",
        "\n",
        "        for i in range(period, len(prices)):\n",
        "            delta = deltas[i - 1]  # The difference between prices[i] and prices[i-1]\n",
        "            if delta > 0:\n",
        "                upval = delta\n",
        "                downval = 0.\n",
        "            else:\n",
        "                upval = 0.\n",
        "                downval = -delta\n",
        "\n",
        "            up = (up*(period-1) + upval)/period\n",
        "            down = (down*(period-1) + downval)/period\n",
        "\n",
        "            rs = up/down\n",
        "            rsi[i] = 100. - 100./(1. + rs)\n",
        "\n",
        "        return rsi\n",
        "\n",
        "    # Calculate EMAs, RSI, and find crossovers individually for each symbol\n",
        "    for symbol in final_df['Symbol'].unique():\n",
        "        symbol_df = final_df[final_df['Symbol'] == symbol].reset_index(drop=True)\n",
        "        prices = symbol_df['Close'].tolist()\n",
        "\n",
        "        # Check if there are enough data points to calculate both EMAs and RSI\n",
        "        if len(prices) < 50:\n",
        "            insufficient_data.append(symbol)\n",
        "            continue\n",
        "\n",
        "        ema20 = calculate_ema(prices, 20)\n",
        "        ema50 = calculate_ema(prices, 50)\n",
        "        rsi = calculate_rsi(prices)\n",
        "\n",
        "        for i in range(1, len(symbol_df)):\n",
        "            if not np.isnan(ema50[i]) and not np.isnan(ema20[i]) and not np.isnan(ema50[i - 1]) and not np.isnan(ema20[i - 1]):\n",
        "                if ema50[i] > ema20[i] and ema50[i - 1] < ema20[i - 1]:\n",
        "                    cross_message = f\"{symbol} - {symbol_df['Date'][i]} - Bull Close: {symbol_df['Close'][i]} - RSI: {rsi[i]:.2f}\"\n",
        "                    results.append([symbol, symbol_df['Date'][i], cross_message])\n",
        "                elif ema50[i] < ema20[i] and ema50[i - 1] > ema20[i - 1]:\n",
        "                    cross_message = f\"{symbol} - {symbol_df['Date'][i]} - Bear Close: {symbol_df['Close'][i]} - RSI: {rsi[i]:.2f}\"\n",
        "                    results.append([symbol, symbol_df['Date'][i], cross_message])\n",
        "\n",
        "    # Convert the results list to a DataFrame\n",
        "    cross_column_filtered = pd.DataFrame(results, columns=['Symbol', 'Date', '50 vs 20 EMA Cross with RSI'])\n",
        "\n",
        "    # Filter the DataFrame to include only those with a '50 vs 20 EMA Cross' within the last 300 days\n",
        "    three_hundred_days_ago = datetime.today() - timedelta(days=300)\n",
        "    cross_column_filtered['Date'] = pd.to_datetime(cross_column_filtered['Date'])\n",
        "    filtered_df = cross_column_filtered[cross_column_filtered['Date'] >= three_hundred_days_ago]\n",
        "\n",
        "    # Group by date and count the number of symbols having crossovers\n",
        "    cross_counts = filtered_df.groupby('Date').count()\n",
        "\n",
        "    # Filter for dates with crossovers of multiple symbols\n",
        "    multiple_cross_dates = cross_counts[cross_counts['Symbol'] > 1].index\n",
        "    multiple_cross_df = filtered_df[filtered_df['Date'].isin(multiple_cross_dates)]\n",
        "\n",
        "    # Filter for dates with crossovers of single symbols\n",
        "    single_cross_dates = cross_counts[cross_counts['Symbol'] == 1].index\n",
        "    single_cross_df = filtered_df[filtered_df['Date'].isin(single_cross_dates)]\n",
        "\n",
        "    # Sort both DataFrames by date\n",
        "    multiple_cross_df = multiple_cross_df.sort_values(by='Date', ascending=False)\n",
        "    single_cross_df = single_cross_df.sort_values(by='Date', ascending=False)\n",
        "\n",
        "    # Save the filtered DataFrames to CSV files\n",
        "    output_multiple_filtered_file_csv = 'ema_cross_results_multiple_symbols_with_rsi.csv'\n",
        "    multiple_cross_df.to_csv(output_multiple_filtered_file_csv, index=False)\n",
        "\n",
        "    output_single_filtered_file_csv = 'ema_cross_results_single_symbols_with_rsi.csv'\n",
        "    single_cross_df.to_csv(output_single_filtered_file_csv, index=False)\n",
        "\n",
        "    # Print the symbols with insufficient data for EMA calculation\n",
        "    # if insufficient_data:\n",
        "    #     print(\"Symbols with insufficient data for EMA calculation:\")\n",
        "    #     for symbol in insufficient_data:\n",
        "    #         print(symbol)\n",
        "# Concatenate the results of multiple and single crossovers\n",
        "combined_df = pd.concat([multiple_cross_df, single_cross_df])\n",
        "\n",
        "# Drop duplicates if there are any\n",
        "combined_df = combined_df.drop_duplicates()\n",
        "\n",
        "# Sort the combined DataFrame by date in descending order\n",
        "combined_df = combined_df.sort_values(by='Date', ascending=False)\n",
        "\n",
        "# Display the combined DataFrame\n",
        "print(\"Combined dates with EMA crossovers:\")\n",
        "print(combined_df[['Symbol', '50 vs 20 EMA Cross with RSI']])\n",
        "\n",
        "# # Save the combined DataFrame to a CSV file\n",
        "# output_combined_file_csv = 'ema_cross_results_combined.csv'\n",
        "# combined_df.to_csv(output_combined_file_csv, index=False)\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import base64\n",
        "\n",
        "# Assuming you have sorted_df DataFrame containing your data\n",
        "# Assuming sorted_df is defined elsewhere in your code\n",
        "\n",
        "try:\n",
        "    # Convert sorted_df to CSV format\n",
        "    csv_data = combined_df.to_csv(index=False)\n",
        "\n",
        "    # Encode the CSV data to Base64\n",
        "    csv_data_base64 = base64.b64encode(csv_data.encode()).decode()\n",
        "\n",
        "    # Define the GitHub repository URL\n",
        "    repo_url = 'https://github.com/iamsrijit/Nepse'\n",
        "\n",
        "    # Define the file name with today's date\n",
        "    file_name = f'EMA_Cross for_{datetime.today().strftime(\"%Y-%m-%d\")}.csv'\n",
        "\n",
        "    # Define your personal access token\n",
        "    token = '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'\n",
        "\n",
        "    # Define the file path in the repository\n",
        "    file_path = f'/{file_name}'\n",
        "\n",
        "    # Define the API URL for uploading files to GitHub\n",
        "    upload_url = f'https://api.github.com/repos/iamsrijit/Nepse/contents{file_path}'\n",
        "\n",
        "    # Prepare the headers with the authorization token\n",
        "    headers = {\n",
        "        'Authorization': f'token {token}',\n",
        "        'Accept': 'application/vnd.github.v3+json'\n",
        "    }\n",
        "\n",
        "    # Prepare the payload with file content\n",
        "    payload = {\n",
        "        'message': f'Upload {file_name}',\n",
        "        'content': csv_data_base64,\n",
        "        'branch': 'main'  # Specify the branch you want to upload to\n",
        "    }\n",
        "\n",
        "    # Send a PUT request to upload the file\n",
        "    response = requests.put(upload_url, headers=headers, json=payload)\n",
        "\n",
        "    # Check the response status\n",
        "    if response.status_code == 200:\n",
        "        print(f'File {file_name} uploaded successfully!')\n",
        "    elif response.status_code == 422:\n",
        "        print(f'Failed to upload {file_name}. Status code: 422 Unprocessable Entity')\n",
        "        print('Error Message:', response.json()['message'])\n",
        "    else:\n",
        "        print(f'Failed to upload {file_name}. Status code: {response.status_code}')\n",
        "        print('Response Content:', response.text)\n",
        "\n",
        "except Exception as e:\n",
        "    print('An error occurred:', e)\n",
        "import requests\n",
        "\n",
        "# Personal Access Token (replace with your actual token)\n",
        "token = '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'\n",
        "\n",
        "# GitHub API endpoint for rate limit information\n",
        "url = 'https://api.github.com/rate_limit'\n",
        "\n",
        "# Send a GET request to the rate limit endpoint\n",
        "response = requests.get(url, auth=('token', token))\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Extract rate limit information from the response\n",
        "    rate_limit_info = response.json()\n",
        "\n",
        "    # Print remaining rate limit\n",
        "    remaining_limit = rate_limit_info['rate']['remaining']\n",
        "    print(f\"Remaining API calls: {remaining_limit}\")\n",
        "\n",
        "    # Print rate limit reset time\n",
        "    reset_time = rate_limit_info['rate']['reset']\n",
        "    print(f\"Rate limit reset time: {reset_time} (Unix timestamp)\")\n",
        "else:\n",
        "    print(f\"Failed to fetch rate limit. Status code: {response.status_code}\")\n",
        "from git import Repo\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Function to identify the latest file for a given pattern\n",
        "def get_latest_file(pattern):\n",
        "    files = [f for f in os.listdir(repo_dir) if re.match(pattern, f)]\n",
        "    if files:\n",
        "        latest_file = max(files)\n",
        "        return latest_file\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Replace with your GitHub repository URL\n",
        "repo_url = 'https://github.com/iamsrijit/Nepse.git'\n",
        "repo_dir = '/content/nepse_new'  # Directory to clone the repository\n",
        "\n",
        "# Check if the directory exists and is not empty\n",
        "if os.path.exists(repo_dir) and os.listdir(repo_dir):\n",
        "    repo = Repo(repo_dir)\n",
        "else:\n",
        "    # Clone the repository\n",
        "    repo = Repo.clone_from(repo_url, repo_dir)\n",
        "\n",
        "# Delete older files for EMA_Cross for_\n",
        "latest_ema_file = get_latest_file('^EMA_Cross for_')\n",
        "if latest_ema_file:\n",
        "    print(f\"Latest EMA file: {latest_ema_file}\")\n",
        "    files_to_delete = [f for f in os.listdir(repo_dir) if re.match('^EMA_Cross for_', f) and f != latest_ema_file]\n",
        "    print(f\"Files to delete for EMA: {files_to_delete}\")\n",
        "    for file_to_delete in files_to_delete:\n",
        "        file_path = os.path.join(repo_dir, file_to_delete)\n",
        "        print(f\"Deleting file: {file_path}\")\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "            repo.index.remove([file_path], working_tree=True)\n",
        "            repo.index.commit(f'Deleted {file_to_delete}')\n",
        "            print(f\"Deleted {file_to_delete}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting {file_to_delete}: {e}\")\n",
        "\n",
        "# Delete older files for espen_\n",
        "latest_espen_file = get_latest_file('^espen_')\n",
        "if latest_espen_file:\n",
        "    print(f\"Latest espn file: {latest_espen_file}\")\n",
        "    files_to_delete = [f for f in os.listdir(repo_dir) if re.match('^espen_', f) and f != latest_espen_file]\n",
        "    print(f\"Files to delete for espn: {files_to_delete}\")\n",
        "    for file_to_delete in files_to_delete:\n",
        "        file_path = os.path.join(repo_dir, file_to_delete)\n",
        "        print(f\"Deleting file: {file_path}\")\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "            repo.index.remove([file_path], working_tree=True)\n",
        "            repo.index.commit(f'Deleted {file_to_delete}')\n",
        "            print(f\"Deleted {file_to_delete}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting {file_to_delete}: {e}\")\n",
        "\n",
        "# Set up the remote repository with authentication\n",
        "origin = repo.remote(name='origin')\n",
        "origin.set_url('https://iamsrijit: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@github.com/iamsrijit/Nepse.git')\n",
        "\n",
        "# Push the changes to GitHub\n",
        "try:\n",
        "    origin.push()\n",
        "    print(\"Pushed changes to GitHub.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error pushing changes to GitHub: {e}\")\n",
        "\n",
        "print(\"Previous files have been deleted from GitHub repository.\")\n"
      ]
    }
  ]
}