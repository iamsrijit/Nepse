# -*- coding: utf-8 -*-
"""ema.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F3uaWp-PxzLmCodzoFRcE2nvKs-Eg_bg
"""

# EMA Crossover Signal Generator - GitHub Integration
# -*- coding: utf-8 -*-
import os
import re
import base64
import requests
import pandas as pd
import numpy as np
from datetime import datetime
from io import StringIO
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
import warnings
warnings.filterwarnings('ignore')

# ===========================
# GITHUB CONFIG
# ===========================
REPO_OWNER = "iamsrijit"
REPO_NAME = "Nepse"
BRANCH = "main"

GH_TOKEN = os.environ.get("GH_TOKEN")
if not GH_TOKEN:
    raise RuntimeError("GH_TOKEN not set in environment")

HEADERS = {"Authorization": f"token {GH_TOKEN}"}

# ===========================
# GITHUB HELPERS
# ===========================
def github_raw(path):
    return f"https://raw.githubusercontent.com/{REPO_OWNER}/{REPO_NAME}/{BRANCH}/{path}"

def upload_to_github(filename, content):
    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/{filename}"
    r = requests.get(url, headers=HEADERS)
    payload = {
        "message": f"Upload {filename}",
        "content": base64.b64encode(content.encode()).decode(),
        "branch": BRANCH
    }
    if r.status_code == 200:
        payload["sha"] = r.json()["sha"]
    res = requests.put(url, headers=HEADERS, json=payload)
    if res.status_code not in (200, 201):
        raise RuntimeError(f"Upload failed: {res.text}")
    print("‚úÖ Uploaded/Overwritten:", filename)

def delete_old_files(prefix, keep_filename):
    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents"
    r = requests.get(url, headers=HEADERS, params={"ref": BRANCH})
    r.raise_for_status()
    for f in r.json():
        name = f["name"]
        if name.startswith(prefix) and name.endswith(".csv") and name != keep_filename:
            del_payload = {
                "message": f"Delete old file {name}",
                "sha": f["sha"],
                "branch": BRANCH
            }
            del_url = f"{url}/{name}"
            res = requests.delete(del_url, headers=HEADERS, json=del_payload)
            if res.status_code == 200:
                print(f"üóëÔ∏è Deleted: {name}")
            else:
                print(f"‚ö†Ô∏è Failed to delete {name}: {res.text}")

def get_latest_espen_csv():
    """Get the latest espen_*.csv file from GitHub"""
    url = f"https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents"
    r = requests.get(url, headers=HEADERS, params={"ref": BRANCH})
    r.raise_for_status()

    # Find all espen_*.csv files
    espen_files = {}
    for f in r.json():
        name = f["name"]
        if name.startswith("espen_") and name.endswith(".csv"):
            m = re.search(r"espen_(\d{4}-\d{2}-\d{2})\.csv", name)
            if m:
                espen_files[m.group(1)] = name

    if not espen_files:
        raise FileNotFoundError("No espen_*.csv file found")

    # Get the latest date
    latest_date = max(espen_files.keys())
    latest_file = espen_files[latest_date]

    print(f"üìÇ Using market data file: {latest_file}")
    return github_raw(latest_file), latest_date

# ===========================
# EMA CROSSOVER STRATEGY CLASS
# ===========================
class EMA_Crossover_Strategy:
    def __init__(self, symbol_data, min_data_points=60):
        """
        Initialize the strategy with symbol-specific data
        min_data_points: Minimum required data points (default 60 for ~3 months)
        """
        self.symbol = symbol_data['Symbol'].iloc[0]
        self.df = symbol_data.copy()
        self.df = self.df.drop('Symbol', axis=1)
        self.min_data_points = min_data_points
        self.prepare_data()

    def prepare_data(self):
        """Prepare and clean the data"""
        # Convert date column to datetime
        if 'Date' in self.df.columns:
            self.df['Date'] = pd.to_datetime(self.df['Date'])
            self.df.set_index('Date', inplace=True)

        # Clean numeric columns - remove commas and convert to float
        numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']

        for col in numeric_columns:
            if col in self.df.columns:
                self.df[col] = self.df[col].astype(str).str.replace(',', '').astype(float)

        # Handle Percent Change if present
        if 'Percent Change' in self.df.columns:
            self.df['Percent Change'] = self.df['Percent Change'].astype(str).str.replace(',', '').astype(float)

        # Sort by date (ascending order - oldest first)
        self.df.sort_index(inplace=True)

    def calculate_indicators(self):
        """Calculate all technical indicators"""
        df = self.df

        # EMAs
        df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()
        df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()

        # ATR (Average True Range)
        df['ATR'] = self.calculate_atr(df, 14)

        # ADX (Average Directional Index)
        df['ADX'] = self.calculate_adx(df, 14)

        # Volume average
        df['Volume_Avg_20'] = df['Volume'].rolling(window=20).mean()

        # Bollinger Bands
        df['BB_Middle'] = df['Close'].rolling(window=20).mean()
        bb_std = df['Close'].rolling(window=20).std()
        df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)
        df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)
        df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']

        # ATR averages for volatility filter
        atr_period = min(50, len(df) - 1)
        if atr_period > 0:
            df['ATR_Avg_50'] = df['ATR'].rolling(window=atr_period).mean()
        else:
            df['ATR_Avg_50'] = df['ATR']

        # EMA slopes (normalized by price)
        df['EMA_20_Slope'] = (df['EMA_20'] - df['EMA_20'].shift(5)) / df['Close'] * 100
        df['EMA_50_Slope'] = (df['EMA_50'] - df['EMA_50'].shift(5)) / df['Close'] * 100

        self.df = df

    def calculate_atr(self, df, period=14):
        """Calculate Average True Range manually"""
        high = df['High']
        low = df['Low']
        close = df['Close']

        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())

        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = tr.rolling(window=period).mean()

        return atr

    def calculate_adx(self, df, period=14):
        """Calculate ADX manually"""
        high = df['High']
        low = df['Low']
        close = df['Close']

        plus_dm = high.diff()
        minus_dm = -low.diff()

        plus_dm[plus_dm < 0] = 0
        minus_dm[minus_dm < 0] = 0

        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)

        atr = tr.rolling(window=period).mean()

        plus_di = 100 * (plus_dm.rolling(window=period).mean() / atr)
        minus_di = 100 * (minus_dm.rolling(window=period).mean() / atr)

        dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)
        adx = dx.rolling(window=period).mean()

        return adx

    def detect_crossovers(self):
        """Detect EMA crossovers"""
        df = self.df

        # Detect crossovers
        df['EMA_Diff'] = df['EMA_20'] - df['EMA_50']
        df['EMA_Diff_Prev'] = df['EMA_Diff'].shift(1)

        # Bullish crossover: 20 EMA crosses above 50 EMA
        df['Bullish_Cross'] = (df['EMA_Diff'] > 0) & (df['EMA_Diff_Prev'] <= 0)

        # Bearish crossover: 20 EMA crosses below 50 EMA
        df['Bearish_Cross'] = (df['EMA_Diff'] < 0) & (df['EMA_Diff_Prev'] >= 0)

        self.df = df

    def apply_filters(self):
        """Apply all filters to remove choppy/sideways signals"""
        df = self.df

        df['Valid_Buy'] = False
        df['Valid_Sell'] = False

        # Calculate minimum index based on available data
        min_index = 50  # Minimum for EMA_50 calculation

        for i in range(len(df)):
            # Skip if we don't have enough data for basic EMAs
            if i < min_index:
                continue

            # Skip if EMAs are not yet calculated
            if pd.isna(df['EMA_20'].iloc[i]) or pd.isna(df['EMA_50'].iloc[i]):
                continue

            # BUY SIGNAL FILTERS
            if df['Bullish_Cross'].iloc[i]:
                valid = True

                # Filter 1: Price above both EMAs (within 1-3 candles)
                price_above_emas = False
                for j in range(i, min(i+4, len(df))):
                    if df['Close'].iloc[j] > df['EMA_20'].iloc[j] and df['Close'].iloc[j] > df['EMA_50'].iloc[j]:
                        price_above_emas = True
                        break

                if not price_above_emas:
                    valid = False

                # Filter 2: Volume above average
                if pd.notna(df['Volume_Avg_20'].iloc[i]) and df['Volume'].iloc[i] < 1.2 * df['Volume_Avg_20'].iloc[i]:
                    valid = False

                # Filter 3: ADX > 20 (strong trend)
                if pd.notna(df['ADX'].iloc[i]) and df['ADX'].iloc[i] < 20:
                    valid = False

                # Filter 4: ATR not declining (volatility check)
                if pd.notna(df['ATR_Avg_50'].iloc[i]) and pd.notna(df['ATR'].iloc[i]):
                    if df['ATR'].iloc[i] < 0.7 * df['ATR_Avg_50'].iloc[i]:
                        valid = False

                # Filter 5: Check for price oscillation between EMAs
                lookback = min(15, i)
                oscillation_count = 0
                for j in range(max(0, i-lookback), i):
                    if (df['Close'].iloc[j] > df['EMA_20'].iloc[j] and df['Close'].iloc[j] < df['EMA_50'].iloc[j]) or \
                       (df['Close'].iloc[j] < df['EMA_20'].iloc[j] and df['Close'].iloc[j] > df['EMA_50'].iloc[j]):
                        oscillation_count += 1

                max_oscillations = max(5, int(lookback * 0.33))
                if oscillation_count > max_oscillations:
                    valid = False

                # Filter 6: Check for recent opposite crossovers
                lookback_cross = min(20, i)
                recent_bearish = df['Bearish_Cross'].iloc[max(0, i-lookback_cross):i].sum()
                if recent_bearish >= 2:
                    valid = False

                # Filter 7: Bollinger Bands not contracting
                bb_lookback = min(50, i)
                if bb_lookback > 0:
                    bb_width_percentile = df['BB_Width'].iloc[max(0, i-bb_lookback):i+1].rank(pct=True).iloc[-1]
                    if bb_width_percentile < 0.2:
                        valid = False

                # Filter 8: EMAs not flat
                if pd.notna(df['EMA_20_Slope'].iloc[i]) and pd.notna(df['EMA_50_Slope'].iloc[i]):
                    if abs(df['EMA_20_Slope'].iloc[i]) < 0.1 and abs(df['EMA_50_Slope'].iloc[i]) < 0.1:
                        valid = False

                df.loc[df.index[i], 'Valid_Buy'] = valid

            # SELL SIGNAL FILTERS
            if df['Bearish_Cross'].iloc[i]:
                valid = True

                # Filter 1: Price below both EMAs
                price_below_emas = False
                for j in range(i, min(i+4, len(df))):
                    if df['Close'].iloc[j] < df['EMA_20'].iloc[j] and df['Close'].iloc[j] < df['EMA_50'].iloc[j]:
                        price_below_emas = True
                        break

                if not price_below_emas:
                    valid = False

                # Filter 2: Volume above average
                if pd.notna(df['Volume_Avg_20'].iloc[i]) and df['Volume'].iloc[i] < 1.2 * df['Volume_Avg_20'].iloc[i]:
                    valid = False

                # Filter 3: ADX > 20
                if pd.notna(df['ADX'].iloc[i]) and df['ADX'].iloc[i] < 20:
                    valid = False

                # Filter 4: ATR not declining
                if pd.notna(df['ATR_Avg_50'].iloc[i]) and pd.notna(df['ATR'].iloc[i]):
                    if df['ATR'].iloc[i] < 0.7 * df['ATR_Avg_50'].iloc[i]:
                        valid = False

                # Filter 5: Check for price oscillation
                lookback = min(15, i)
                oscillation_count = 0
                for j in range(max(0, i-lookback), i):
                    if (df['Close'].iloc[j] > df['EMA_20'].iloc[j] and df['Close'].iloc[j] < df['EMA_50'].iloc[j]) or \
                       (df['Close'].iloc[j] < df['EMA_20'].iloc[j] and df['Close'].iloc[j] > df['EMA_50'].iloc[j]):
                        oscillation_count += 1

                max_oscillations = max(5, int(lookback * 0.33))
                if oscillation_count > max_oscillations:
                    valid = False

                # Filter 6: Check for recent opposite crossovers
                lookback_cross = min(20, i)
                recent_bullish = df['Bullish_Cross'].iloc[max(0, i-lookback_cross):i].sum()
                if recent_bullish >= 2:
                    valid = False

                # Filter 7: Bollinger Bands not contracting
                bb_lookback = min(50, i)
                if bb_lookback > 0:
                    bb_width_percentile = df['BB_Width'].iloc[max(0, i-bb_lookback):i+1].rank(pct=True).iloc[-1]
                    if bb_width_percentile < 0.2:
                        valid = False

                # Filter 8: EMAs not flat
                if pd.notna(df['EMA_20_Slope'].iloc[i]) and pd.notna(df['EMA_50_Slope'].iloc[i]):
                    if abs(df['EMA_20_Slope'].iloc[i]) < 0.1 and abs(df['EMA_50_Slope'].iloc[i]) < 0.1:
                        valid = False

                df.loc[df.index[i], 'Valid_Sell'] = valid

        self.df = df

    def generate_signals(self):
        """Main method to generate signals"""
        try:
            # Check if we have minimum required data
            if len(self.df) < self.min_data_points:
                return {
                    'symbol': self.symbol,
                    'buy_count': 0,
                    'sell_count': 0,
                    'buy_signals': pd.DataFrame(),
                    'sell_signals': pd.DataFrame(),
                    'success': False,
                    'error': f'Insufficient data: {len(self.df)} days (minimum {self.min_data_points} required)',
                    'data_points': len(self.df)
                }

            self.calculate_indicators()
            self.detect_crossovers()
            self.apply_filters()

            # Create signals dataframe - Reset index to preserve Date column
            df_with_date = self.df.reset_index()

            buy_signals = df_with_date[df_with_date['Valid_Buy'] == True].copy()
            sell_signals = df_with_date[df_with_date['Valid_Sell'] == True].copy()

            # Add symbol and signal type columns
            buy_signals['Symbol'] = self.symbol
            sell_signals['Symbol'] = self.symbol
            buy_signals['Signal_Type'] = 'BUY'
            sell_signals['Signal_Type'] = 'SELL'

            return {
                'symbol': self.symbol,
                'buy_count': len(buy_signals),
                'sell_count': len(sell_signals),
                'buy_signals': buy_signals,
                'sell_signals': sell_signals,
                'success': True,
                'data_points': len(self.df)
            }
        except Exception as e:
            return {
                'symbol': self.symbol,
                'buy_count': 0,
                'sell_count': 0,
                'buy_signals': pd.DataFrame(),
                'sell_signals': pd.DataFrame(),
                'success': False,
                'error': str(e),
                'data_points': len(self.df)
            }


def process_symbol(symbol_data, min_data_points=60):
    """Process a single symbol - used for parallel execution"""
    try:
        strategy = EMA_Crossover_Strategy(symbol_data, min_data_points=min_data_points)
        result = strategy.generate_signals()
        return result
    except Exception as e:
        return {
            'symbol': symbol_data['Symbol'].iloc[0],
            'buy_count': 0,
            'sell_count': 0,
            'buy_signals': pd.DataFrame(),
            'sell_signals': pd.DataFrame(),
            'success': False,
            'error': str(e),
            'data_points': len(symbol_data)
        }


# ===========================
# MAIN EXECUTION
# ===========================
if __name__ == "__main__":
    print("="*80)
    print("EMA CROSSOVER SIGNAL GENERATOR - GitHub Integration")
    print("="*80)

    # Get latest espen CSV from GitHub
    csv_url, latest_market_date = get_latest_espen_csv()

    # Load data from GitHub
    print(f"\nüì• Downloading data from GitHub...")
    response = requests.get(csv_url)
    csv_content = response.text

    # Find the header line
    lines = csv_content.strip().split('\n')
    header_line = None
    data_start_index = 0

    for i, line in enumerate(lines):
        if 'Symbol' in line and 'Date' in line and 'Close' in line:
            header_line = line
            data_start_index = i
            break

    if header_line is None:
        raise ValueError("Could not find header line with Symbol, Date, Close columns")

    # Reconstruct CSV with header at top
    if data_start_index > 0:
        data_lines = lines[:data_start_index]
        reconstructed_csv = header_line + '\n' + '\n'.join(data_lines)
    else:
        reconstructed_csv = csv_content

    # Parse the CSV - try tab separator first, then comma
    try:
        df = pd.read_csv(StringIO(reconstructed_csv), sep='\t')
        if len(df.columns) == 1:
            df = pd.read_csv(StringIO(reconstructed_csv), sep=',')
    except:
        df = pd.read_csv(StringIO(reconstructed_csv), sep=',')

    # Clean column names
    df.columns = df.columns.str.strip()

    print(f"üìä Loaded {len(df)} rows")
    print(f"üìã Columns: {list(df.columns)}")

    # Prepare data
    numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = df[col].astype(str).str.replace(',', '').astype(float)

    if 'Percent Change' in df.columns:
        df['Percent Change'] = df['Percent Change'].astype(str).str.replace(',', '').astype(float)

    df['Date'] = pd.to_datetime(df['Date'])

    # Get unique symbols
    symbols = df['Symbol'].unique()
    symbol_counts = df.groupby('Symbol').size()

    print(f"\nüìà Total symbols: {len(symbols)}")
    print(f"   Symbols with >= 60 days: {(symbol_counts >= 60).sum()}")
    print(f"   Symbols with < 60 days: {(symbol_counts < 60).sum()}")

    # Group data by symbol
    symbol_groups = [group for _, group in df.groupby('Symbol')]

    print(f"\n‚öôÔ∏è Processing {len(symbol_groups)} symbols using {multiprocessing.cpu_count()} CPU cores...")

    # Process in parallel with progress bar
    results = Parallel(n_jobs=-1)(
        delayed(process_symbol)(symbol_data, min_data_points=60)
        for symbol_data in tqdm(symbol_groups, desc="Analyzing symbols")
    )

    # Generate reports
    print("\nüìù Generating reports...")
    all_buy_signals = []
    all_sell_signals = []

    successful_count = 0
    insufficient_data_count = 0

    for result in results:
        if result['success']:
            successful_count += 1
            if len(result['buy_signals']) > 0:
                all_buy_signals.append(result['buy_signals'])
            if len(result['sell_signals']) > 0:
                all_sell_signals.append(result['sell_signals'])
        else:
            if 'Insufficient data' in result.get('error', ''):
                insufficient_data_count += 1

    # Combine all signals
    all_buy_df = pd.concat(all_buy_signals, ignore_index=True) if all_buy_signals else pd.DataFrame()
    all_sell_df = pd.concat(all_sell_signals, ignore_index=True) if all_sell_signals else pd.DataFrame()
    all_signals_df = pd.concat([all_buy_df, all_sell_df], ignore_index=True)

    if len(all_signals_df) > 0:
        all_signals_df['Date'] = pd.to_datetime(all_signals_df['Date'])
        all_signals_df = all_signals_df.sort_values(['Date', 'Symbol'], ascending=[False, True])

    # Print summary
    print("\n" + "="*80)
    print("ANALYSIS SUMMARY")
    print("="*80)
    print(f"Total Symbols: {len(symbols)}")
    print(f"Successful: {successful_count}")
    print(f"Insufficient Data: {insufficient_data_count}")
    print(f"\nTotal Buy Signals: {len(all_buy_df)}")
    print(f"Total Sell Signals: {len(all_sell_df)}")
    print(f"Total Signals: {len(all_signals_df)}")

    # Show latest signals (last 7 days)
    if len(all_signals_df) > 0:
        print("\n" + "="*80)
        print("LATEST SIGNALS (LAST 7 DAYS)")
        print("="*80)
        latest_date = all_signals_df['Date'].max()
        cutoff_date = latest_date - pd.Timedelta(days=7)
        recent_signals = all_signals_df[all_signals_df['Date'] >= cutoff_date].copy()

        if len(recent_signals) > 0:
            display_cols = ['Date', 'Symbol', 'Signal_Type', 'Close', 'EMA_20', 'EMA_50', 'ADX']
            display_cols = [col for col in display_cols if col in recent_signals.columns]
            print(recent_signals[display_cols].head(30).to_string(index=False))
        else:
            print("No signals in the last 7 days")

    # Export and upload to GitHub
    if len(all_signals_df) > 0:
        print("\nüì§ Uploading results to GitHub...")

        # Select relevant columns for export
        export_cols = ['Date', 'Symbol', 'Signal_Type', 'Close', 'EMA_20', 'EMA_50',
                      'Volume', 'Volume_Avg_20', 'ADX', 'ATR']
        export_cols = [col for col in export_cols if col in all_signals_df.columns]

        # Create CSV content
        csv_content = all_signals_df[export_cols].to_csv(index=False)

        # Upload with date in filename
        signals_file = f"EMA_CROSSOVER_SIGNALS_{latest_market_date}.csv"
        upload_to_github(signals_file, csv_content)

        # Delete old signal files
        delete_old_files("EMA_CROSSOVER_SIGNALS_", signals_file)

        print(f"\n‚úÖ Results uploaded: {signals_file}")
    else:
        print("\n‚ö†Ô∏è No signals found - nothing to upload")

    print("\n" + "="*80)
    print("‚úÖ ANALYSIS COMPLETE!")
    print("="*80)