# -*- coding: utf-8 -*-
"""nepse_autp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v54qgCaBc-MBcBjC5VdpDbG23qmigpmM
"""

import subprocess
import sys
import os
import requests
import base64
import pandas as pd
import numpy as np
from datetime import datetime
from nepse_scraper import Nepse_scraper
from joblib import Parallel, delayed
import matplotlib.pyplot as plt
from git import Repo
import re

# Install required packages if missing
packages = ["nepse-scraper", "xlsxwriter", "gitpython", "pandas","matplotlib","joblib"]
subprocess.check_call([sys.executable, "-m", "pip", "install"] + packages)

# ------------------------ Scrape NEPSE Data ------------------------
request_obj = Nepse_scraper()
today_price = request_obj.get_today_price()
content_data = today_price.get('content', [])

filtered_data = []
for item in content_data:
    symbol = item.get('symbol', '')
    date = item.get('businessDate', '')
    open_price = item.get('openPrice', 0)
    high_price = item.get('highPrice', 0)
    low_price = item.get('lowPrice', 0)
    close_price = item.get('closePrice', 0)
    volume_daily = item.get('totalTradedQuantity', 0)
    high_52w = item.get('fiftyTwoWeekHigh', np.nan)
    low_52w = item.get('fiftyTwoWeekLow', np.nan)

    percent_change = ((close_price - open_price) / open_price * 100) if open_price else 0

    filtered_data.append({
        'Symbol': symbol,
        'Date': date,
        'Open': open_price,
        'High': high_price,
        'Low': low_price,
        'Close': close_price,
        'Percent Change': round(percent_change, 2),
        'Volume': volume_daily,
        '52W_High': high_52w,
        '52W_Low': low_52w
    })

first = pd.DataFrame(filtered_data)

# Calculate percent from 52-week high/low
first['Pct_from_52W_High'] = 100 * first['Close'] / first['52W_High']
first['Pct_from_52W_Low'] = 100 * first['Close'] / first['52W_Low']
first['Pct_from_52W_High_Sign'] = first['Pct_from_52W_High'].apply(lambda x: f"+{x-100:.2f}%" if x >= 100 else f"-{100-x:.2f}%")
first['Pct_from_52W_Low_Sign'] = first['Pct_from_52W_Low'].apply(lambda x: f"+{x-100:.2f}%" if x >= 100 else f"-{100-x:.2f}%")

# Save daily CSV
today_day_name = datetime.now().strftime('%A')
daily_file_name = f"nepse_{today_day_name}.csv"
first.to_csv(daily_file_name, index=False)
print(f"Daily data saved to '{daily_file_name}'")

# ------------------------ Merge with latest GitHub file ------------------------
def get_latest_file_url(repo_url):
    response = requests.get(repo_url)
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')
    file_links = soup.find_all('a', href=True)
    file_urls = {}
    for link in file_links:
        file_name = link['href']
        if file_name.endswith('.csv'):
            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', file_name)
            if date_match:
                file_date = date_match.group(1)
                file_urls[file_date] = repo_url.replace('/tree/', '/raw/') + '/' + file_name
    if not file_urls:
        raise ValueError("No CSV files found in the repository.")
    latest_file_date = max(file_urls.keys())
    latest_file_url = file_urls[latest_file_date]
    return latest_file_url

repo_url = 'https://github.com/iamsrijit/Nepse/tree/main'
try:
    latest_file_url = get_latest_file_url(repo_url)
    latest_file_url = latest_file_url.replace('/iamsrijit/Nepse/blob/main/', '/')
    secondss = pd.read_csv(latest_file_url)
    secondss.columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'Volume', '52W_High', '52W_Low']
except Exception as e:
    print("No previous GitHub CSV found or error:", e)
    secondss = pd.DataFrame()

# Merge first and secondss, exclude mutual funds
dfs = [first, secondss]
exclude_symbols = [
    'SAEF', 'SEF', 'CMF1', 'NICGF', 'NBF2', 'CMF2', 'NMB50', 'SIGS2', 'NICBF',
    'SFMF', 'LUK', 'SLCF', 'KEF', 'SBCF', 'PSF', 'NIBSF2', 'NICSF', 'RMF1',
    'NBF3', 'MMF1', 'KDBY', 'NICFC', 'GIBF1', 'NSIF2', 'SAGF', 'NIBLGF',
    'SFEF', 'PRSF', 'C30MF', 'SIGS3', 'RMF2', 'LVF2', 'H8020', 'NICGF2',
    'NIBLSTF', 'KSY', 'NBLD87', 'PBD88', 'OTHERS','HIDCLP','NIMBPO','MUTUAL',
    'CIT','ILI','LEMF','NIBLPF','INVESTMENT','SENFLOAT','HEIP','SBID83','NICAD8283'
]

finall_df = pd.DataFrame()
for df in dfs:
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%m/%d/%Y')
        df.dropna(subset=['Date'], inplace=True)

combined_df = pd.concat(dfs, ignore_index=True, join='outer')
combined_df = combined_df[~combined_df['Symbol'].isin(exclude_symbols)]
combined_df['Date'] = pd.to_datetime(combined_df['Date'], format='%m/%d/%Y')

for symbol in combined_df['Symbol'].unique():
    symbol_df = combined_df[combined_df['Symbol'] == symbol]
    symbol_df = symbol_df.sort_values('Date', ascending=False)
    symbol_df = symbol_df.drop_duplicates(subset=['Date'], keep='first')
    finall_df = pd.concat([finall_df, symbol_df], ignore_index=True)

output_file_name = 'combined_data.csv'
finall_df.to_csv(output_file_name, index=False)
print(finall_df.head())

# ------------------------ EMA, RSI, Crossovers ------------------------
data = finall_df.copy()
data = data.sort_values(['Symbol','Date']).reset_index(drop=True)
data['Close'] = data['Close'].astype(str).str.replace(',', '').astype('float32')
data['Volume'] = data['Volume'].astype(str).str.replace(',', '').replace('-', np.nan).astype('float32')

def calculate_rsi(data, period=14):
    delta = data['Close'].diff()
    gain = (delta.where(delta>0,0)).rolling(window=period, min_periods=1).mean()
    loss = (-delta.where(delta<0,0)).rolling(window=period, min_periods=1).mean()
    rs = gain/loss
    return 100 - (100/(1+rs))

def process_symbol(symbol, group):
    group['EMA_20'] = group['Close'].ewm(span=20, adjust=False).mean()
    group['EMA_50'] = group['Close'].ewm(span=50, adjust=False).mean()
    group['RSI'] = calculate_rsi(group)
    group['30D_Avg_Volume'] = group['Volume'].rolling(30, min_periods=1).mean()
    group['Slope_20'] = group['EMA_20'].diff(5)/5
    group['Slope_50'] = group['EMA_50'].diff(5)/5
    group['Crossover'] = (group['EMA_20'] > group['EMA_50']) & (group['EMA_20'].shift(1) <= group['EMA_50'].shift(1))
    valid_crossovers = group[
        group['Crossover'] &
        (group['Slope_20']>group['Slope_50']) &
        (group['RSI'].between(30,70)) &
        (group['Volume']>=0.3*group['30D_Avg_Volume']) &
        (group['Close']>group['Close'].rolling(60,min_periods=1).max().shift(1)*0.95)
    ]
    return valid_crossovers

results = Parallel(n_jobs=-1)(delayed(process_symbol)(symbol, group) for symbol, group in data.groupby('Symbol'))
all_valid_crossovers = pd.concat(results).reset_index(drop=True)

# Add 52-week percentage columns
all_valid_crossovers['Pct_from_52W_High'] = 100 * all_valid_crossovers['Close']/all_valid_crossovers['52W_High']
all_valid_crossovers['Pct_from_52W_Low'] = 100 * all_valid_crossovers['Close']/all_valid_crossovers['52W_Low']
all_valid_crossovers['Pct_from_52W_High_Sign'] = all_valid_crossovers['Pct_from_52W_High'].apply(lambda x: f"+{x-100:.2f}%" if x>=100 else f"-{100-x:.2f}%")
all_valid_crossovers['Pct_from_52W_Low_Sign'] = all_valid_crossovers['Pct_from_52W_Low'].apply(lambda x: f"+{x-100:.2f}%" if x>=100 else f"-{100-x:.2f}%")

all_valid_crossovers.to_csv('valid_ema_crossovers.csv', index=False)

latest_crossovers = all_valid_crossovers.sort_values('Date', ascending=False).drop_duplicates('Symbol')
latest_crossovers.to_csv('latest_valid_ema_crossovers.csv', index=False)
print(latest_crossovers[['Symbol','Date','Close']])

# ------------------------ Upload to GitHub ------------------------
token = os.getenv("GH_TOKEN")

def upload_file_to_github(df, filename):
    csv_b64 = base64.b64encode(df.to_csv(index=False).encode()).decode()
    headers = {'Authorization': f'token {token}'}
    upload_url = f'https://api.github.com/repos/iamsrijit/Nepse/contents/{filename}'
    response = requests.get(upload_url, headers=headers)
    sha = response.json()['sha'] if response.status_code==200 else None
    payload = {'message': f'Upload {filename}', 'content': csv_b64, 'branch': 'main'}
    if sha:
        payload['sha'] = sha
    resp = requests.put(upload_url, headers=headers, json=payload)
    print(f"{filename} upload status: {resp.status_code}")

upload_file_to_github(finall_df, f'espen_{datetime.today().strftime("%Y-%m-%d")}.csv')
upload_file_to_github(latest_crossovers, f'EMA_Cross_for_{datetime.today().strftime("%Y-%m-%d")}.csv')

# ------------------------ Remove old files from GitHub repo ------------------------
repo_dir = './nepse_repo'
if not os.path.exists(repo_dir):
    Repo.clone_from('https://github.com/iamsrijit/Nepse.git', repo_dir)
repo = Repo(repo_dir)

all_files = os.listdir(repo_dir)
ema_pattern = r'^EMA_Cross_for_\d{4}-\d{2}-\d{2}\.csv$'
espen_pattern = r'^espen_\d{4}-\d{2}-\d{2}\.csv$'

latest_ema = max([f for f in all_files if re.match(ema_pattern, f)], default=None)
latest_espen = max([f for f in all_files if re.match(espen_pattern, f)], default=None)

files_to_delete = []
if latest_ema:
    files_to_delete += [f for f in all_files if re.match(ema_pattern, f) and f!=latest_ema]
if latest_espen:
    files_to_delete += [f for f in all_files if re.match(espen_pattern, f) and f!=latest_espen]

for file in files_to_delete:
    os.remove(os.path.join(repo_dir,file))
    repo.index.remove([file], working_tree=True)

try:
    repo.git.add(update=True)
    repo.index.commit('Remove old data files')
    origin = repo.remote(name='origin')
    origin.set_url(f'https://x-access-token:{token}@github.com/iamsrijit/Nepse.git')
    origin.push()
except Exception as e:
    print(f"Error pushing changes: {e}")