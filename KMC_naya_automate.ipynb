{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EfjJB0z6MDiQ",
        "outputId": "badd975e-7a97-4ef7-d3b5-d9e8ae59f371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nepse-scraper\n",
            "  Downloading nepse_scraper-0.1.7.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from nepse-scraper) (2.32.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from nepse-scraper) (2.0.7)\n",
            "Collecting wasmtime (from nepse-scraper)\n",
            "  Downloading wasmtime-24.0.0-py3-none-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting retrying (from nepse-scraper)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->nepse-scraper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->nepse-scraper) (3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->nepse-scraper) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->nepse-scraper) (1.16.0)\n",
            "Requirement already satisfied: importlib-resources>=5.10 in /usr/local/lib/python3.10/dist-packages (from wasmtime->nepse-scraper) (6.4.5)\n",
            "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading wasmtime-24.0.0-py3-none-manylinux1_x86_64.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nepse-scraper\n",
            "  Building wheel for nepse-scraper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nepse-scraper: filename=nepse_scraper-0.1.7-py3-none-any.whl size=10151 sha256=fcb5cba56615f954140b61997732250d172242b39470f8cab387021571bdd063\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/f8/66/819db2d7e2ecbdd45cf61624eebb79f362c758750202721aaf\n",
            "Successfully built nepse-scraper\n",
            "Installing collected packages: wasmtime, retrying, nepse-scraper\n",
            "Successfully installed nepse-scraper-0.1.7 retrying-1.3.4 wasmtime-24.0.0\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 smmap-5.0.1\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.43)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.11)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nepse-scraper\n",
        "!pip install xlsxwriter\n",
        "!pip install gitpython\n",
        "!pip install gitpython pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pol02ZhnAk6f"
      },
      "source": [
        "**Daily Nepse Scrapping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InYE7C5V5ZZc"
      },
      "outputs": [],
      "source": [
        "from nepse_scraper import Nepse_scraper\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create an object from the Nepse_scraper class\n",
        "request_obj = Nepse_scraper()\n",
        "\n",
        "# Get today's price from NEPSE\n",
        "today_price = request_obj.get_today_price()\n",
        "\n",
        "# Extract the 'content' section from the response\n",
        "content_data = today_price.get('content', [])\n",
        "\n",
        "# Initialize an empty list to store filtered data\n",
        "filtered_data = []\n",
        "\n",
        "# Define the column names\n",
        "columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close','Volume']\n",
        "\n",
        "# Iterate over each item in the 'content' section\n",
        "for item in content_data:\n",
        "    symbol = item.get('symbol', '')\n",
        "    date = item.get('businessDate', '')\n",
        "    open_price = item.get('openPrice', '')\n",
        "    high_price = item.get('highPrice', '')\n",
        "    low_price = item.get('lowPrice', '')\n",
        "    close_price = item.get('closePrice', '')\n",
        "    Volume_daily = item.get('totalTradedValue', '')\n",
        "\n",
        "    # Append the extracted values to the filtered data list\n",
        "    filtered_data.append({\n",
        "        'Symbol': symbol,\n",
        "        'Date': date,\n",
        "        'Open': open_price,\n",
        "        'High': high_price,\n",
        "        'Low': low_price,\n",
        "        'Close': close_price,\n",
        "        'Volume': Volume_daily\n",
        "    })\n",
        "\n",
        "# Create DataFrame from filtered data\n",
        "first = pd.DataFrame(filtered_data)\n",
        "\n",
        "# Check if DataFrame has data\n",
        "if not first.empty:\n",
        "    # Display DataFrame\n",
        "    print(first)\n",
        "\n",
        "    # Get today's day name\n",
        "    today_day_name = datetime.now().strftime('%A')\n",
        "\n",
        "    # Save DataFrame to CSV with today's day name in the filename\n",
        "    file_name = f'nepse_{today_day_name}.csv'\n",
        "    first.to_csv(file_name, index=False)\n",
        "\n",
        "    print(f\"Data saved to '{file_name}'\")\n",
        "else:\n",
        "    print(\"No data available to create DataFrame.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH83QpxFO5LO"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sk1qFFu_Msx"
      },
      "outputs": [],
      "source": [
        "# from nepse_scraper import Nepse_scraper\n",
        "# import pandas as pd\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Create an object from the Nepse_scraper class\n",
        "# request_obj = Nepse_scraper()\n",
        "\n",
        "# # Get today's price from NEPSE\n",
        "# today_price = request_obj.get_today_price()\n",
        "\n",
        "# # Extract the 'content' section from the response\n",
        "# content_data = today_price.get('content', [])\n",
        "\n",
        "# # Debugging: Print the first few items to inspect structure\n",
        "# for index, item in enumerate(content_data[:5]):  # Limit to first 5 items for brevity\n",
        "#     print(f\"Item {index + 1}: {item}\")\n",
        "\n",
        "# # Initialize an empty list to store filtered data\n",
        "# filtered_data = []\n",
        "\n",
        "# # Define the column names\n",
        "# columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "# # Assuming we've identified the correct key after inspecting the structure\n",
        "# for item in content_data:\n",
        "#     symbol = item.get('symbol', '')\n",
        "#     date = item.get('businessDate', '')\n",
        "#     open_price = item.get('openPrice', '')\n",
        "#     high_price = item.get('highPrice', '')\n",
        "#     low_price = item.get('lowPrice', '')\n",
        "#     close_price = item.get('closePrice', '')\n",
        "\n",
        "#     # Replace 'volume' with the correct key after inspecting the items\n",
        "#     volume_daily = item.get('totalTradeQuantity', '')  # Placeholder key; update this after inspection\n",
        "\n",
        "#     # Append the extracted values to the filtered data list\n",
        "#     filtered_data.append({\n",
        "#         'Symbol': symbol,\n",
        "#         'Date': date,\n",
        "#         'Open': open_price,\n",
        "#         'High': high_price,\n",
        "#         'Low': low_price,\n",
        "#         'Close': close_price,\n",
        "#         'Volume': volume_daily\n",
        "#     })\n",
        "\n",
        "# # Create DataFrame from filtered data\n",
        "# first = pd.DataFrame(filtered_data)\n",
        "\n",
        "# # Check if DataFrame has data\n",
        "# if not first.empty:\n",
        "#     # Display DataFrame\n",
        "#     print(first)\n",
        "\n",
        "#     # Get today's day name\n",
        "#     today_day_name = datetime.now().strftime('%A')\n",
        "\n",
        "#     # Save DataFrame to CSV with today's day name in the filename\n",
        "#     file_name = f'nepse_{today_day_name}.csv'\n",
        "#     first.to_csv(file_name, index=False)\n",
        "\n",
        "#     print(f\"Data saved to '{file_name}'\")\n",
        "# else:\n",
        "#     print(\"No data available to create DataFrame.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk8UWhO1Ta5M"
      },
      "outputs": [],
      "source": [
        "# first.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqyPMMBgAuV7"
      },
      "source": [
        "**access nepse data till July 31**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSD_iQXYJsp2"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# file_url='https://github.com/iamsrijit/Nepse/raw/b58db55fbbc5aaca4fe949f3ca12d177ec62ec7b/2024%20AUGUST%205.xlsx'\n",
        "# # # # Read data from the URL\n",
        "# secondss = pd.read_excel(file_url)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-4M0DqTEpqY"
      },
      "outputs": [],
      "source": [
        "# secondss.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Fz5o4-whJmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to get the latest file URL\n",
        "def get_latest_file_url(repo_url):\n",
        "    # Send a GET request to the GitHub repository\n",
        "    response = requests.get(repo_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all links to files in the repository\n",
        "    file_links = soup.find_all('a', href=True)\n",
        "\n",
        "    # Extract file names and corresponding URLs\n",
        "    file_urls = {}\n",
        "    for link in file_links:\n",
        "        file_name = link['href']\n",
        "        if file_name.endswith('.csv'):\n",
        "            date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', file_name)\n",
        "            if date_match:\n",
        "                file_date = date_match.group(1)\n",
        "                file_urls[file_date] = repo_url.replace('/tree/', '/raw/') + '/' + file_name\n",
        "\n",
        "    if not file_urls:\n",
        "        raise ValueError(\"No CSV files found in the repository.\")\n",
        "\n",
        "    # Get the latest file URL based on the date\n",
        "    latest_file_date = max(file_urls.keys())\n",
        "    latest_file_url = file_urls[latest_file_date]\n",
        "    print(\"Latest file date:\", latest_file_date)\n",
        "    print(\"Latest file URL:\", latest_file_url)\n",
        "    return latest_file_url\n",
        "\n",
        "# Replace with the actual GitHub repository URL\n",
        "repo_url = 'https://github.com/iamsrijit/Nepse/tree/main'\n",
        "\n",
        "try:\n",
        "    # Get the latest file URL\n",
        "    latest_file_url = get_latest_file_url(repo_url)\n",
        "\n",
        "    # Correct the file URL\n",
        "    latest_file_url = latest_file_url.replace('/iamsrijit/Nepse/blob/main/', '/')\n",
        "\n",
        "    # Read data from the latest file\n",
        "    secondss = pd.read_csv(latest_file_url)\n",
        "\n",
        "    # Assuming the column names are the same as in your example\n",
        "    secondss.columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close','Volume']\n",
        "\n",
        "    print(secondss.head())\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR6bYdIJ8C-j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tn06vxvyhFt"
      },
      "source": [
        "**merging ma date anusar milaunu paryoo**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HROkozh3qNxZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'first' and 'secondss' are your DataFrames\n",
        "dfs = [first, secondss]\n",
        "\n",
        "# Create an empty DataFrame to store the final results\n",
        "final_df = pd.DataFrame()\n",
        "\n",
        "for df in dfs:\n",
        "    try:\n",
        "        if 'Date' not in df.columns:\n",
        "            print(f\"'Date' column not found in DataFrame.\")\n",
        "            continue\n",
        "\n",
        "        # Convert the 'Date' column to datetime format and drop rows with invalid dates\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%m/%d/%Y')\n",
        "        df.dropna(subset=['Date'], inplace=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing DataFrame: {e}\")\n",
        "\n",
        "# Combine all the DataFrames\n",
        "if not dfs:\n",
        "    print(\"No valid data to process.\")\n",
        "else:\n",
        "    combined_df = pd.concat(dfs, ignore_index=True, join='outer')\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], format='%m/%d/%Y')\n",
        "\n",
        "    # Iterate over each unique symbol\n",
        "    for symbol in combined_df['Symbol'].unique():\n",
        "        symbol_df = combined_df[combined_df['Symbol'] == symbol]\n",
        "        symbol_df = symbol_df.sort_values(by=['Date'], ascending=False)\n",
        "        symbol_df = symbol_df.drop_duplicates(subset=['Date'], keep='first')\n",
        "\n",
        "        # Initialize columns 'G' and 'H' as None\n",
        "        symbol_df['G'] = None\n",
        "        symbol_df['H'] = None\n",
        "\n",
        "        # Append to the final DataFrame\n",
        "        final_df = pd.concat([final_df, symbol_df], ignore_index=True)\n",
        "\n",
        "    # Save the final DataFrame to a CSV file\n",
        "    output_file_name = 'combined_data.csv'\n",
        "\n",
        "    # Write the headers to the CSV file\n",
        "    with open(output_file_name, 'w') as f:\n",
        "        headers = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'G', 'H','Volume']\n",
        "        f.write(','.join(headers) + '\\n')\n",
        "\n",
        "    # Append the final DataFrame without headers to the CSV file\n",
        "    final_df.to_csv(output_file_name, mode='a', index=False, header=False)\n",
        "\n",
        "    # Optional: print the first few rows of the final DataFrame\n",
        "    # print(final_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w-oZgM_s8x--"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJHZm2pa80_s"
      },
      "source": [
        "**exclude mutual funds **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f7vVS9Ho80ce",
        "outputId": "81368662-2e52-4864-b64a-0db5447c5269"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-8709d0167965>:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  symbol_df['G'] = None\n",
            "<ipython-input-10-8709d0167965>:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  symbol_df['H'] = None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'first' and 'secondss' are your DataFrames\n",
        "dfs = [first, secondss]\n",
        "\n",
        "# Create an empty DataFrame to store the final results\n",
        "final_df = pd.DataFrame()\n",
        "\n",
        "# List of symbols to exclude\n",
        "exclude_symbols = [\n",
        "    'SAEF', 'SEF', 'CMF1', 'NICGF', 'NBF2', 'CMF2', 'NMB50', 'SIGS2', 'NICBF',\n",
        "    'SFMF', 'LUK', 'SLCF', 'KEF', 'SBCF', 'PSF', 'NIBSF2', 'NICSF', 'RMF1',\n",
        "    'NBF3', 'MMF1', 'KDBY', 'NICFC', 'GIBF1', 'NSIF2', 'SAGF', 'NIBLGF',\n",
        "    'SFEF', 'PRSF', 'C30MF', 'SIGS3', 'RMF2', 'LVF2', 'H8020', 'NICGF2',\n",
        "    'NIBLSTF', 'KSY' ,'NBLD87', 'PBD88', 'OTHERS','HIDCLP','NIMBPO','MUTUAL',\n",
        "    'CIT','ILI','LEMF','NIBLPF','INVESTMENT','SENFLOAT','HEIP' ,'SBID83','NICAD8283'\n",
        "]\n",
        "\n",
        "for df in dfs:\n",
        "    try:\n",
        "        if 'Date' not in df.columns:\n",
        "            print(f\"'Date' column not found in DataFrame.\")\n",
        "            continue\n",
        "\n",
        "        # Convert the 'Date' column to datetime format and drop rows with invalid dates\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%m/%d/%Y')\n",
        "        df.dropna(subset=['Date'], inplace=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing DataFrame: {e}\")\n",
        "\n",
        "# Combine all the DataFrames\n",
        "if not dfs:\n",
        "    print(\"No valid data to process.\")\n",
        "else:\n",
        "    combined_df = pd.concat(dfs, ignore_index=True, join='outer')\n",
        "\n",
        "    # Filter out the excluded symbols\n",
        "    combined_df = combined_df[~combined_df['Symbol'].isin(exclude_symbols)]\n",
        "\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], format='%m/%d/%Y')\n",
        "\n",
        "    # Iterate over each unique symbol\n",
        "    for symbol in combined_df['Symbol'].unique():\n",
        "        symbol_df = combined_df[combined_df['Symbol'] == symbol]\n",
        "        symbol_df = symbol_df.sort_values(by=['Date'], ascending=False)\n",
        "        symbol_df = symbol_df.drop_duplicates(subset=['Date'], keep='first')\n",
        "\n",
        "        # Initialize columns 'G' and 'H' as None\n",
        "        symbol_df['G'] = None\n",
        "        symbol_df['H'] = None\n",
        "\n",
        "        # Append to the final DataFrame\n",
        "        final_df = pd.concat([final_df, symbol_df], ignore_index=True)\n",
        "\n",
        "    # Save the final DataFrame to a CSV file\n",
        "    output_file_name = 'combined_data.csv'\n",
        "\n",
        "    # Write the headers to the CSV file\n",
        "    with open(output_file_name, 'w') as f:\n",
        "        headers = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'G', 'H','Volume']\n",
        "        f.write(','.join(headers) + '\\n')\n",
        "\n",
        "    # Append the final DataFrame without headers to the CSV file\n",
        "    final_df.to_csv(output_file_name, mode='a', index=False, header=False)\n",
        "\n",
        "    # Optional: print the first few rows of the final DataFrame\n",
        "    # print(final_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KB4_TVPrqT-m"
      },
      "outputs": [],
      "source": [
        "# print(final_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iAE05Vp9cTQ"
      },
      "source": [
        "**THIS CODE NEED TO BE TESTED FOR LARGE DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qcYe8ZrFcSqO",
        "outputId": "68f53c62-bcc4-4f2a-b9d1-a20df6d34cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to upload espen_2024-09-13.csv. Status code: 201\n",
            "Response Content: {\"content\":{\"name\":\"espen_2024-09-13.csv\",\"path\":\"espen_2024-09-13.csv\",\"sha\":\"49d6a21ad481c2afddd6200dc4eb204f1c6399f0\",\"size\":8608287,\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/contents/espen_2024-09-13.csv?ref=main\",\"html_url\":\"https://github.com/iamsrijit/Nepse/blob/main/espen_2024-09-13.csv\",\"git_url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/blobs/49d6a21ad481c2afddd6200dc4eb204f1c6399f0\",\"download_url\":\"https://raw.githubusercontent.com/iamsrijit/Nepse/main/espen_2024-09-13.csv\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/iamsrijit/Nepse/contents/espen_2024-09-13.csv?ref=main\",\"git\":\"https://api.github.com/repos/iamsrijit/Nepse/git/blobs/49d6a21ad481c2afddd6200dc4eb204f1c6399f0\",\"html\":\"https://github.com/iamsrijit/Nepse/blob/main/espen_2024-09-13.csv\"}},\"commit\":{\"sha\":\"ad4d699ac9ffd72efd2352ed17082625783566fb\",\"node_id\":\"C_kwDOMHuaNtoAKGFkNGQ2OTlhYzlmZmQ3MmVmZDIzNTJlZDE3MDgyNjI1NzgzNTY2ZmI\",\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/commits/ad4d699ac9ffd72efd2352ed17082625783566fb\",\"html_url\":\"https://github.com/iamsrijit/Nepse/commit/ad4d699ac9ffd72efd2352ed17082625783566fb\",\"author\":{\"name\":\"srijit pokharel\",\"email\":\"46723334+iamsrijit@users.noreply.github.com\",\"date\":\"2024-09-13T09:28:39Z\"},\"committer\":{\"name\":\"srijit pokharel\",\"email\":\"46723334+iamsrijit@users.noreply.github.com\",\"date\":\"2024-09-13T09:28:39Z\"},\"tree\":{\"sha\":\"6b32293f5caee8d19c0b32f683382d90962e772b\",\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/trees/6b32293f5caee8d19c0b32f683382d90962e772b\"},\"message\":\"Upload espen_2024-09-13.csv\",\"parents\":[{\"sha\":\"9c362327d76cc6ebae303acf4f3b5b58586e6f86\",\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/commits/9c362327d76cc6ebae303acf4f3b5b58586e6f86\",\"html_url\":\"https://github.com/iamsrijit/Nepse/commit/9c362327d76cc6ebae303acf4f3b5b58586e6f86\"}],\"verification\":{\"verified\":false,\"reason\":\"unsigned\",\"signature\":null,\"payload\":null}}}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "import base64\n",
        "\n",
        "# Assuming you have sorted_df DataFrame containing your data\n",
        "# Assuming sorted_df is defined elsewhere in your code\n",
        "\n",
        "try:\n",
        "    # Convert sorted_df to CSV format\n",
        "    csv_data = final_df.to_csv(index=False)\n",
        "\n",
        "    # Encode the CSV data to Base64\n",
        "    csv_data_base64 = base64.b64encode(csv_data.encode()).decode()\n",
        "\n",
        "    # Define the GitHub repository URL\n",
        "    repo_url = 'https://github.com/iamsrijit/Nepse'\n",
        "\n",
        "    # Define the file name with today's date\n",
        "    file_name = f'espen_{datetime.today().strftime(\"%Y-%m-%d\")}.csv'\n",
        "\n",
        "    # Define your personal access token\n",
        "    token = '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'\n",
        "\n",
        "    # Define the file path in the repository\n",
        "    file_path = f'/{file_name}'\n",
        "\n",
        "    # Define the API URL for uploading files to GitHub\n",
        "    upload_url = f'https://api.github.com/repos/iamsrijit/Nepse/contents{file_path}'\n",
        "\n",
        "    # Prepare the headers with the authorization token\n",
        "    headers = {\n",
        "        'Authorization': f'token {token}',\n",
        "        'Accept': 'application/vnd.github.v3+json'\n",
        "    }\n",
        "\n",
        "    # Prepare the payload with file content\n",
        "    payload = {\n",
        "        'message': f'Upload {file_name}',\n",
        "        'content': csv_data_base64,\n",
        "        'branch': 'main'  # Specify the branch you want to upload to\n",
        "    }\n",
        "\n",
        "    # Send a PUT request to upload the file\n",
        "    response = requests.put(upload_url, headers=headers, json=payload)\n",
        "\n",
        "    # Check the response status\n",
        "    if response.status_code == 200:\n",
        "        print(f'File {file_name} uploaded successfully!')\n",
        "    elif response.status_code == 422:\n",
        "        print(f'Failed to upload {file_name}. Status code: 422 Unprocessable Entity')\n",
        "        print('Error Message:', response.json()['message'])\n",
        "    else:\n",
        "        print(f'Failed to upload {file_name}. Status code: {response.status_code}')\n",
        "        print('Response Content:', response.text)\n",
        "\n",
        "except Exception as e:\n",
        "    print('An error occurred:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9_EesUcE2S8"
      },
      "source": [
        "**Status code: 201 is successful**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeeGAnZLMzJi"
      },
      "source": [
        "**bold8/1/2024 text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MIELwNMGzT-5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9I_o8qGwzUkz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n0hRstmzzUn9"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# # Sample DataFrame (final_df) should have columns ['Symbol', 'Date', 'Close']\n",
        "# # Make sure final_df is sorted by 'Symbol' and 'Date' (in ascending order)\n",
        "# final_df = final_df.sort_values(by=['Symbol', 'Date'], ascending=[True, True])\n",
        "\n",
        "# # Check if the number of unique symbols is less than 50\n",
        "# if final_df['Symbol'].nunique() < 50:\n",
        "#     print(\"The number of unique symbols is less than 50. Please provide more data.\")\n",
        "# else:\n",
        "#     # Initialize an empty list to store results for each symbol\n",
        "#     results = []\n",
        "#     insufficient_data = []\n",
        "\n",
        "#     # Function to calculate EMA\n",
        "#     def calculate_ema(prices, span):\n",
        "#         ema = np.full(len(prices), np.nan)\n",
        "#         if len(prices) < span:\n",
        "#             return ema\n",
        "#         ema[span-1] = sum(prices[:span]) / span\n",
        "#         multiplier = 2 / (span + 1)\n",
        "#         for i in range(span, len(prices)):\n",
        "#             ema[i] = (prices[i] - ema[i-1]) * multiplier + ema[i-1]\n",
        "#         return ema\n",
        "\n",
        "#     # Calculate EMAs and find crossovers individually for each symbol\n",
        "#     for symbol in final_df['Symbol'].unique():\n",
        "#         symbol_df = final_df[final_df['Symbol'] == symbol].reset_index(drop=True)\n",
        "#         prices = symbol_df['Close'].tolist()\n",
        "\n",
        "#         # Check if there are enough data points to calculate both EMAs\n",
        "#         if len(prices) < 50:\n",
        "#             insufficient_data.append(symbol)\n",
        "#             continue\n",
        "\n",
        "#         ema20 = calculate_ema(prices, 20)\n",
        "#         ema50 = calculate_ema(prices, 50)\n",
        "\n",
        "#         for i in range(1, len(symbol_df)):\n",
        "#             if not np.isnan(ema50[i]) and not np.isnan(ema20[i]) and not np.isnan(ema50[i - 1]) and not np.isnan(ema20[i - 1]):\n",
        "#                 if ema50[i] > ema20[i] and ema50[i - 1] < ema20[i - 1]:\n",
        "#                     cross_message = f\"{symbol} - {symbol_df['Date'][i]} - Bull Close: {symbol_df['Close'][i]}\"\n",
        "#                     results.append([symbol, symbol_df['Date'][i], cross_message])\n",
        "#                 elif ema50[i] < ema20[i] and ema50[i - 1] > ema20[i - 1]:\n",
        "#                     cross_message = f\"{symbol} - {symbol_df['Date'][i]} - Bear Close: {symbol_df['Close'][i]}\"\n",
        "#                     results.append([symbol, symbol_df['Date'][i], cross_message])\n",
        "\n",
        "#     # Convert the results list to a DataFrame\n",
        "#     cross_column_filtered = pd.DataFrame(results, columns=['Symbol', 'Date', '50 vs 20 EMA Cross'])\n",
        "\n",
        "#     # Filter the DataFrame to include only those with a '50 vs 20 EMA Cross' within the last 300 days\n",
        "#     three_hundred_days_ago = datetime.today() - timedelta(days=300)\n",
        "#     cross_column_filtered['Date'] = pd.to_datetime(cross_column_filtered['Date'])\n",
        "#     filtered_df = cross_column_filtered[cross_column_filtered['Date'] >= three_hundred_days_ago]\n",
        "\n",
        "#     # Group by date and count the number of symbols having crossovers\n",
        "#     cross_counts = filtered_df.groupby('Date').count()\n",
        "\n",
        "#     # Filter for dates with crossovers of multiple symbols\n",
        "#     multiple_cross_dates = cross_counts[cross_counts['Symbol'] > 1].index\n",
        "#     multiple_cross_df = filtered_df[filtered_df['Date'].isin(multiple_cross_dates)]\n",
        "\n",
        "#     # Filter for dates with crossovers of single symbols\n",
        "#     single_cross_dates = cross_counts[cross_counts['Symbol'] == 1].index\n",
        "#     single_cross_df = filtered_df[filtered_df['Date'].isin(single_cross_dates)]\n",
        "\n",
        "#     # Sort both DataFrames by date\n",
        "#     multiple_cross_df = multiple_cross_df.sort_values(by='Date', ascending=False)\n",
        "#     single_cross_df = single_cross_df.sort_values(by='Date', ascending=False)\n",
        "\n",
        "#     # Save the filtered DataFrames to CSV files\n",
        "#     output_multiple_filtered_file_csv = 'ema_cross_results_multiple_symbols.csv'\n",
        "#     multiple_cross_df.to_csv(output_multiple_filtered_file_csv, index=False)\n",
        "\n",
        "#     output_single_filtered_file_csv = 'ema_cross_results_single_symbols.csv'\n",
        "#     single_cross_df.to_csv(output_single_filtered_file_csv, index=False)\n",
        "\n",
        "#     # Print the symbols with insufficient data for EMA calculation\n",
        "#     # if insufficient_data:\n",
        "#     #     print(\"Symbols with insufficient data for EMA calculation:\")\n",
        "#     #     for symbol in insufficient_data:\n",
        "#     #         print(symbol)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn6IQlZ_jHM6"
      },
      "source": [
        "**WITH RSI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RQ384XbRDHBZ",
        "outputId": "9729d237-b4f5-41dc-ab39-bdb36b821032"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-8ddf4000c750>:34: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  rs = up/down\n",
            "<ipython-input-14-8ddf4000c750>:50: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  rs = up/down\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Sample DataFrame (final_df) should have columns ['Symbol', 'Date', 'Close']\n",
        "# Make sure final_df is sorted by 'Symbol' and 'Date' (in ascending order)\n",
        "final_df = final_df.sort_values(by=['Symbol', 'Date'], ascending=[True, True])\n",
        "\n",
        "# Check if the number of unique symbols is less than 50\n",
        "if final_df['Symbol'].nunique() < 50:\n",
        "    print(\"The number of unique symbols is less than 50. Please provide more data.\")\n",
        "else:\n",
        "    # Initialize an empty list to store results for each symbol\n",
        "    results = []\n",
        "    insufficient_data = []\n",
        "\n",
        "    # Function to calculate EMA\n",
        "    def calculate_ema(prices, span):\n",
        "        ema = np.full(len(prices), np.nan)\n",
        "        if len(prices) < span:\n",
        "            return ema\n",
        "        ema[span-1] = sum(prices[:span]) / span\n",
        "        multiplier = 2 / (span + 1)\n",
        "        for i in range(span, len(prices)):\n",
        "            ema[i] = (prices[i] - ema[i-1]) * multiplier + ema[i-1]\n",
        "        return ema\n",
        "\n",
        "    # Function to calculate RSI\n",
        "    def calculate_rsi(prices, period=14):\n",
        "        deltas = np.diff(prices)\n",
        "        seed = deltas[:period+1]\n",
        "        up = seed[seed >= 0].sum()/period\n",
        "        down = -seed[seed < 0].sum()/period\n",
        "        rs = up/down\n",
        "        rsi = np.zeros_like(prices)\n",
        "        rsi[:period] = 100. - 100./(1. + rs)\n",
        "\n",
        "        for i in range(period, len(prices)):\n",
        "            delta = deltas[i - 1]  # The difference between prices[i] and prices[i-1]\n",
        "            if delta > 0:\n",
        "                upval = delta\n",
        "                downval = 0.\n",
        "            else:\n",
        "                upval = 0.\n",
        "                downval = -delta\n",
        "\n",
        "            up = (up*(period-1) + upval)/period\n",
        "            down = (down*(period-1) + downval)/period\n",
        "\n",
        "            rs = up/down\n",
        "            rsi[i] = 100. - 100./(1. + rs)\n",
        "\n",
        "        return rsi\n",
        "\n",
        "    # Calculate EMAs, RSI, and find crossovers individually for each symbol\n",
        "    for symbol in final_df['Symbol'].unique():\n",
        "        symbol_df = final_df[final_df['Symbol'] == symbol].reset_index(drop=True)\n",
        "        prices = symbol_df['Close'].tolist()\n",
        "\n",
        "        # Check if there are enough data points to calculate both EMAs and RSI\n",
        "        if len(prices) < 50:\n",
        "            insufficient_data.append(symbol)\n",
        "            continue\n",
        "\n",
        "        ema20 = calculate_ema(prices, 20)\n",
        "        ema50 = calculate_ema(prices, 50)\n",
        "        rsi = calculate_rsi(prices)\n",
        "\n",
        "        for i in range(1, len(symbol_df)):\n",
        "            if not np.isnan(ema50[i]) and not np.isnan(ema20[i]) and not np.isnan(ema50[i - 1]) and not np.isnan(ema20[i - 1]):\n",
        "                if ema50[i] > ema20[i] and ema50[i - 1] < ema20[i - 1]:\n",
        "                    cross_message = f\"{symbol} - {symbol_df['Date'][i]} - Bull Close: {symbol_df['Close'][i]} - RSI: {rsi[i]:.2f}\"\n",
        "                    results.append([symbol, symbol_df['Date'][i], cross_message])\n",
        "                elif ema50[i] < ema20[i] and ema50[i - 1] > ema20[i - 1]:\n",
        "                    cross_message = f\"{symbol} - {symbol_df['Date'][i]} - Bear Close: {symbol_df['Close'][i]} - RSI: {rsi[i]:.2f}\"\n",
        "                    results.append([symbol, symbol_df['Date'][i], cross_message])\n",
        "\n",
        "    # Convert the results list to a DataFrame\n",
        "    cross_column_filtered = pd.DataFrame(results, columns=['Symbol', 'Date', '50 vs 20 EMA Cross with RSI'])\n",
        "\n",
        "    # Filter the DataFrame to include only those with a '50 vs 20 EMA Cross' within the last 300 days\n",
        "    three_hundred_days_ago = datetime.today() - timedelta(days=300)\n",
        "    cross_column_filtered['Date'] = pd.to_datetime(cross_column_filtered['Date'])\n",
        "    filtered_df = cross_column_filtered[cross_column_filtered['Date'] >= three_hundred_days_ago]\n",
        "\n",
        "    # Group by date and count the number of symbols having crossovers\n",
        "    cross_counts = filtered_df.groupby('Date').count()\n",
        "\n",
        "    # Filter for dates with crossovers of multiple symbols\n",
        "    multiple_cross_dates = cross_counts[cross_counts['Symbol'] > 1].index\n",
        "    multiple_cross_df = filtered_df[filtered_df['Date'].isin(multiple_cross_dates)]\n",
        "\n",
        "    # Filter for dates with crossovers of single symbols\n",
        "    single_cross_dates = cross_counts[cross_counts['Symbol'] == 1].index\n",
        "    single_cross_df = filtered_df[filtered_df['Date'].isin(single_cross_dates)]\n",
        "\n",
        "    # Sort both DataFrames by date\n",
        "    multiple_cross_df = multiple_cross_df.sort_values(by='Date', ascending=False)\n",
        "    single_cross_df = single_cross_df.sort_values(by='Date', ascending=False)\n",
        "\n",
        "    # Save the filtered DataFrames to CSV files\n",
        "    output_multiple_filtered_file_csv = 'ema_cross_results_multiple_symbols_with_rsi.csv'\n",
        "    multiple_cross_df.to_csv(output_multiple_filtered_file_csv, index=False)\n",
        "\n",
        "    output_single_filtered_file_csv = 'ema_cross_results_single_symbols_with_rsi.csv'\n",
        "    single_cross_df.to_csv(output_single_filtered_file_csv, index=False)\n",
        "\n",
        "    # Print the symbols with insufficient data for EMA calculation\n",
        "    # if insufficient_data:\n",
        "    #     print(\"Symbols with insufficient data for EMA calculation:\")\n",
        "    #     for symbol in insufficient_data:\n",
        "    #         print(symbol)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R4yD-geZbREC",
        "outputId": "f7d3798f-aa6b-49f3-9001-bfe738146bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined dates with EMA crossovers:\n",
            "       Symbol                        50 vs 20 EMA Cross with RSI\n",
            "1778    SMATA  SMATA - 2024-09-02 00:00:00 - Bull Close: 952....\n",
            "1903     SWMF  SWMF - 2024-09-02 00:00:00 - Bull Close: 864.0...\n",
            "974      MKJC  MKJC - 2024-09-01 00:00:00 - Bull Close: 507.0...\n",
            "480      GILB  GILB - 2024-09-01 00:00:00 - Bull Close: 1295....\n",
            "278      CYCL  CYCL - 2024-08-29 00:00:00 - Bull Close: 1495....\n",
            "...       ...                                                ...\n",
            "1349    PMHPL  PMHPL - 2023-11-26 00:00:00 - Bear Close: 212....\n",
            "1820     SMJC  SMJC - 2023-11-26 00:00:00 - Bear Close: 266.0...\n",
            "16    ADBLD83  ADBLD83 - 2023-11-23 00:00:00 - Bear Close: 10...\n",
            "1319     PCBL  PCBL - 2023-11-23 00:00:00 - Bear Close: 193.4...\n",
            "2019     USHL  USHL - 2023-11-22 00:00:00 - Bull Close: 306.0...\n",
            "\n",
            "[774 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Concatenate the results of multiple and single crossovers\n",
        "combined_df = pd.concat([multiple_cross_df, single_cross_df])\n",
        "\n",
        "# Drop duplicates if there are any\n",
        "combined_df = combined_df.drop_duplicates()\n",
        "\n",
        "# Sort the combined DataFrame by date in descending order\n",
        "combined_df = combined_df.sort_values(by='Date', ascending=False)\n",
        "\n",
        "# Display the combined DataFrame\n",
        "print(\"Combined dates with EMA crossovers:\")\n",
        "print(combined_df[['Symbol', '50 vs 20 EMA Cross with RSI']])\n",
        "\n",
        "# # Save the combined DataFrame to a CSV file\n",
        "# output_combined_file_csv = 'ema_cross_results_combined.csv'\n",
        "# combined_df.to_csv(output_combined_file_csv, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OzR00wQA0Xfh",
        "outputId": "516b30ec-2ce2-4b72-8d2a-e2b3f88dc223"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"combined_df\",\n  \"rows\": 774,\n  \"fields\": [\n    {\n      \"column\": \"Symbol\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 242,\n        \"samples\": [\n          \"RBCL\",\n          \"BNHC\",\n          \"UAIL\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2023-11-22 00:00:00\",\n        \"max\": \"2024-09-02 00:00:00\",\n        \"num_unique_values\": 162,\n        \"samples\": [\n          \"2023-11-27 00:00:00\",\n          \"2024-02-13 00:00:00\",\n          \"2024-01-09 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"50 vs 20 EMA Cross with RSI\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 774,\n        \"samples\": [\n          \"MFIL - 2024-03-20 00:00:00 - Bear Close: 524.0 - RSI: 54.37\",\n          \"CZBIL - 2024-01-30 00:00:00 - Bull Close: 170.0 - RSI: 42.36\",\n          \"ADBL - 2023-12-21 00:00:00 - Bear Close: 256.0 - RSI: 60.32\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "combined_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-15621c77-1dfb-43a2-8d05-124ce0b3ee2e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Date</th>\n",
              "      <th>50 vs 20 EMA Cross with RSI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1778</th>\n",
              "      <td>SMATA</td>\n",
              "      <td>2024-09-02</td>\n",
              "      <td>SMATA - 2024-09-02 00:00:00 - Bull Close: 952....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1903</th>\n",
              "      <td>SWMF</td>\n",
              "      <td>2024-09-02</td>\n",
              "      <td>SWMF - 2024-09-02 00:00:00 - Bull Close: 864.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>974</th>\n",
              "      <td>MKJC</td>\n",
              "      <td>2024-09-01</td>\n",
              "      <td>MKJC - 2024-09-01 00:00:00 - Bull Close: 507.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>GILB</td>\n",
              "      <td>2024-09-01</td>\n",
              "      <td>GILB - 2024-09-01 00:00:00 - Bull Close: 1295....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>CYCL</td>\n",
              "      <td>2024-08-29</td>\n",
              "      <td>CYCL - 2024-08-29 00:00:00 - Bull Close: 1495....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15621c77-1dfb-43a2-8d05-124ce0b3ee2e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-15621c77-1dfb-43a2-8d05-124ce0b3ee2e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-15621c77-1dfb-43a2-8d05-124ce0b3ee2e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5206d634-67d7-4cff-9a16-871b89cde921\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5206d634-67d7-4cff-9a16-871b89cde921')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5206d634-67d7-4cff-9a16-871b89cde921 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     Symbol       Date                        50 vs 20 EMA Cross with RSI\n",
              "1778  SMATA 2024-09-02  SMATA - 2024-09-02 00:00:00 - Bull Close: 952....\n",
              "1903   SWMF 2024-09-02  SWMF - 2024-09-02 00:00:00 - Bull Close: 864.0...\n",
              "974    MKJC 2024-09-01  MKJC - 2024-09-01 00:00:00 - Bull Close: 507.0...\n",
              "480    GILB 2024-09-01  GILB - 2024-09-01 00:00:00 - Bull Close: 1295....\n",
              "278    CYCL 2024-08-29  CYCL - 2024-08-29 00:00:00 - Bull Close: 1495...."
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z5LjWlcGCBWl"
      },
      "outputs": [],
      "source": [
        "# combined_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ksuG3h8IBZ_t"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Define RSI Calculation\n",
        "# def calculate_rsi(df, window=14):\n",
        "#     delta = df['Close'].diff(1)\n",
        "#     gain = (delta.where(delta > 0, 0)).fillna(0)\n",
        "#     loss = (-delta.where(delta < 0, 0)).fillna(0)\n",
        "\n",
        "#     avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
        "#     avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "#     rs = avg_gain / avg_loss\n",
        "#     rsi = 100 - (100 / (1 + rs))\n",
        "#     return rsi\n",
        "\n",
        "# # Define MACD Calculation\n",
        "# def calculate_macd(df, fastperiod=12, slowperiod=26, signalperiod=9):\n",
        "#     df['MACD'] = df['Close'].ewm(span=fastperiod, adjust=False).mean() - df['Close'].ewm(span=slowperiod, adjust=False).mean()\n",
        "#     df['MACD Signal'] = df['MACD'].ewm(span=signalperiod, adjust=False).mean()\n",
        "#     return df\n",
        "\n",
        "# # Define Bollinger Bands Calculation\n",
        "# def calculate_bollinger_bands(df, window=20, num_of_std=2):\n",
        "#     df['Middle Band'] = df['Close'].rolling(window=window).mean()\n",
        "#     df['Upper Band'] = df['Middle Band'] + (df['Close'].rolling(window=window).std() * num_of_std)\n",
        "#     df['Lower Band'] = df['Middle Band'] - (df['Close'].rolling(window=window).std() * num_of_std)\n",
        "#     return df\n",
        "\n",
        "# # Define Sell Signal Generation\n",
        "# def generate_sell_signal(row):\n",
        "#     sell_signal = False\n",
        "#     if (row['EMA Crossover'] < 0 and  # 20 EMA is below 50 EMA\n",
        "#         row['3-Day Avg Volume'] > row['Previous Volume'] and  # Increasing volume\n",
        "#         row['RSI'] > 70 and  # Overbought condition\n",
        "#         row['MACD'] < row['MACD Signal'] and  # MACD bearish crossover\n",
        "#         row['Close'] > row['Upper Band']  # Price touches/exceeds upper Bollinger Band\n",
        "#     ):\n",
        "#         sell_signal = True\n",
        "#     return sell_signal\n",
        "\n",
        "# # Load your stock data into final_df\n",
        "# # Example structure: final_df = pd.read_csv('your_stock_data.csv')\n",
        "\n",
        "# # Calculating 20 EMA and 50 EMA\n",
        "# final_df['20 EMA'] = final_df['Close'].ewm(span=20, adjust=False).mean()\n",
        "# final_df['50 EMA'] = final_df['Close'].ewm(span=50, adjust=False).mean()\n",
        "\n",
        "# # Identify the crossover\n",
        "# final_df['EMA Crossover'] = final_df['20 EMA'] - final_df['50 EMA']\n",
        "\n",
        "# # Calculate RSI, MACD, and Bollinger Bands\n",
        "# final_df['RSI'] = calculate_rsi(final_df)\n",
        "# final_df = calculate_macd(final_df)\n",
        "# final_df = calculate_bollinger_bands(final_df)\n",
        "\n",
        "# # Calculate 3-Day Average Volume\n",
        "# final_df['3-Day Avg Volume'] = final_df['Volume'].rolling(window=3).mean()\n",
        "# final_df['Previous Volume'] = final_df['Volume'].shift(1)\n",
        "\n",
        "# # Generate Sell Signals\n",
        "# final_df['Sell Signal'] = final_df.apply(generate_sell_signal, axis=1)\n",
        "\n",
        "# # Filter for rows with Sell Signal == True\n",
        "# sell_signals_df = final_df[final_df['Sell Signal'] == True]\n",
        "\n",
        "# # Assuming 'combined_df' is your DataFrame with EMA cross results\n",
        "# # Example: combined_df = pd.DataFrame(columns=['Symbol', 'Date', '50 vs 20 EMA Cross'])\n",
        "\n",
        "# # Merge with combined_df on Symbol and Date\n",
        "# combined_df = pd.merge(combined_df, sell_signals_df[['Symbol', 'Date', 'Sell Signal']], on=['Symbol', 'Date'], how='left')\n",
        "\n",
        "# # Keep only rows where Sell Signal is True\n",
        "# combined_df = combined_df[combined_df['Sell Signal'] == True]\n",
        "\n",
        "# # Drop the Sell Signal column for final output\n",
        "# combined_df = combined_df.drop(columns=['Sell Signal'])\n",
        "\n",
        "# # Sort the combined DataFrame by date in descending order\n",
        "# combined_df = combined_df.sort_values(by='Date', ascending=False)\n",
        "\n",
        "# # Save the results\n",
        "# output_combined_file_csv = 'ema_cross_results_combined.csv'\n",
        "# combined_df.to_csv(output_combined_file_csv, index=False)\n",
        "\n",
        "# # Display the DataFrame with sell signals\n",
        "# print(\"Combined dates with EMA crossovers and Sell Signals:\")\n",
        "# print(combined_df[['Symbol', 'Date', '50 vs 20 EMA Cross']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OUbVV5PzKy0m"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# files.download(output_combined_file_csv )\n",
        "\n",
        "# =VALUE(TRIM(MID(C2, FIND(\"Close: \", C2) + LEN(\"Close: \"), LEN(C2))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N6R8PiMMIDPG",
        "outputId": "743cdb4f-88c5-4648-aedb-39299bf5bdc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to upload EMA_Cross for_2024-09-13.csv. Status code: 201\n",
            "Response Content: {\"content\":{\"name\":\"EMA_Cross for_2024-09-13.csv\",\"path\":\"EMA_Cross for_2024-09-13.csv\",\"sha\":\"2d76aede515eeaf6913fc68b614f701fe6827d0e\",\"size\":59446,\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/contents/EMA_Cross%20for_2024-09-13.csv?ref=main\",\"html_url\":\"https://github.com/iamsrijit/Nepse/blob/main/EMA_Cross%20for_2024-09-13.csv\",\"git_url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/blobs/2d76aede515eeaf6913fc68b614f701fe6827d0e\",\"download_url\":\"https://raw.githubusercontent.com/iamsrijit/Nepse/main/EMA_Cross%20for_2024-09-13.csv\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/iamsrijit/Nepse/contents/EMA_Cross%20for_2024-09-13.csv?ref=main\",\"git\":\"https://api.github.com/repos/iamsrijit/Nepse/git/blobs/2d76aede515eeaf6913fc68b614f701fe6827d0e\",\"html\":\"https://github.com/iamsrijit/Nepse/blob/main/EMA_Cross%20for_2024-09-13.csv\"}},\"commit\":{\"sha\":\"dbb33e49d27da5ae23e636602f2d2d425a474b9e\",\"node_id\":\"C_kwDOMHuaNtoAKGRiYjMzZTQ5ZDI3ZGE1YWUyM2U2MzY2MDJmMmQyZDQyNWE0NzRiOWU\",\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/commits/dbb33e49d27da5ae23e636602f2d2d425a474b9e\",\"html_url\":\"https://github.com/iamsrijit/Nepse/commit/dbb33e49d27da5ae23e636602f2d2d425a474b9e\",\"author\":{\"name\":\"srijit pokharel\",\"email\":\"46723334+iamsrijit@users.noreply.github.com\",\"date\":\"2024-09-13T09:28:46Z\"},\"committer\":{\"name\":\"srijit pokharel\",\"email\":\"46723334+iamsrijit@users.noreply.github.com\",\"date\":\"2024-09-13T09:28:46Z\"},\"tree\":{\"sha\":\"a7511a3e3e8246f943f2c6c658e03d7da7568f64\",\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/trees/a7511a3e3e8246f943f2c6c658e03d7da7568f64\"},\"message\":\"Upload EMA_Cross for_2024-09-13.csv\",\"parents\":[{\"sha\":\"ad4d699ac9ffd72efd2352ed17082625783566fb\",\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/commits/ad4d699ac9ffd72efd2352ed17082625783566fb\",\"html_url\":\"https://github.com/iamsrijit/Nepse/commit/ad4d699ac9ffd72efd2352ed17082625783566fb\"}],\"verification\":{\"verified\":false,\"reason\":\"unsigned\",\"signature\":null,\"payload\":null}}}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "import base64\n",
        "\n",
        "# Assuming you have sorted_df DataFrame containing your data\n",
        "# Assuming sorted_df is defined elsewhere in your code\n",
        "\n",
        "try:\n",
        "    # Convert sorted_df to CSV format\n",
        "    csv_data = combined_df.to_csv(index=False)\n",
        "\n",
        "    # Encode the CSV data to Base64\n",
        "    csv_data_base64 = base64.b64encode(csv_data.encode()).decode()\n",
        "\n",
        "    # Define the GitHub repository URL\n",
        "    repo_url = 'https://github.com/iamsrijit/Nepse'\n",
        "\n",
        "    # Define the file name with today's date\n",
        "    file_name = f'EMA_Cross for_{datetime.today().strftime(\"%Y-%m-%d\")}.csv'\n",
        "\n",
        "    # Define your personal access token\n",
        "    token = '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'\n",
        "\n",
        "    # Define the file path in the repository\n",
        "    file_path = f'/{file_name}'\n",
        "\n",
        "    # Define the API URL for uploading files to GitHub\n",
        "    upload_url = f'https://api.github.com/repos/iamsrijit/Nepse/contents{file_path}'\n",
        "\n",
        "    # Prepare the headers with the authorization token\n",
        "    headers = {\n",
        "        'Authorization': f'token {token}',\n",
        "        'Accept': 'application/vnd.github.v3+json'\n",
        "    }\n",
        "\n",
        "    # Prepare the payload with file content\n",
        "    payload = {\n",
        "        'message': f'Upload {file_name}',\n",
        "        'content': csv_data_base64,\n",
        "        'branch': 'main'  # Specify the branch you want to upload to\n",
        "    }\n",
        "\n",
        "    # Send a PUT request to upload the file\n",
        "    response = requests.put(upload_url, headers=headers, json=payload)\n",
        "\n",
        "    # Check the response status\n",
        "    if response.status_code == 200:\n",
        "        print(f'File {file_name} uploaded successfully!')\n",
        "    elif response.status_code == 422:\n",
        "        print(f'Failed to upload {file_name}. Status code: 422 Unprocessable Entity')\n",
        "        print('Error Message:', response.json()['message'])\n",
        "    else:\n",
        "        print(f'Failed to upload {file_name}. Status code: {response.status_code}')\n",
        "        print('Response Content:', response.text)\n",
        "\n",
        "except Exception as e:\n",
        "    print('An error occurred:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "neFIjrW_dYEo"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Assuming filtered_df is already defined and contains the necessary data\n",
        "# # filtered_df = ...\n",
        "\n",
        "# # Function to extract symbol and create URL\n",
        "# def create_url(entry):\n",
        "#     symbol = entry.split(' ')[0]\n",
        "#     return f\"https://nepsealpha.com/trading/chart?symbol={symbol}\"\n",
        "\n",
        "# # Apply the function to the dataframe\n",
        "# combined_df['URL'] = combined_df['50 vs 20 EMA Cross'].apply(create_url)\n",
        "\n",
        "# # Print the resulting URLs\n",
        "# #print(filtered_df['URL'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qtLX95wEdRK2"
      },
      "outputs": [],
      "source": [
        "# print(combined_df['URL'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl4AIf84-knQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pyeeufMuXj9y",
        "outputId": "8287f344-1881-4e1c-ca42-17e6f24f2b92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remaining API calls: 4998\n",
            "Rate limit reset time: 1726223319 (Unix timestamp)\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Personal Access Token (replace with your actual token)\n",
        "token = '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'\n",
        "\n",
        "# GitHub API endpoint for rate limit information\n",
        "url = 'https://api.github.com/rate_limit'\n",
        "\n",
        "# Send a GET request to the rate limit endpoint\n",
        "response = requests.get(url, auth=('token', token))\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Extract rate limit information from the response\n",
        "    rate_limit_info = response.json()\n",
        "\n",
        "    # Print remaining rate limit\n",
        "    remaining_limit = rate_limit_info['rate']['remaining']\n",
        "    print(f\"Remaining API calls: {remaining_limit}\")\n",
        "\n",
        "    # Print rate limit reset time\n",
        "    reset_time = rate_limit_info['rate']['reset']\n",
        "    print(f\"Rate limit reset time: {reset_time} (Unix timestamp)\")\n",
        "else:\n",
        "    print(f\"Failed to fetch rate limit. Status code: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yMT29SDDl9Nr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONDn9XjDbFF1"
      },
      "source": [
        "**Delete from Github**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ENk1fxSFSqnJ"
      },
      "outputs": [],
      "source": [
        "# from git import Repo\n",
        "\n",
        "# # Replace with your GitHub repository URL\n",
        "# repo_url = 'https://github.com/iamsrijit/Nepse.git'\n",
        "# repo_dir = '/content/new/new/new/new'  # New directory to clone the repository\n",
        "\n",
        "# # Clone the repository\n",
        "# repo = Repo.clone_from(repo_url, repo_dir)\n",
        "\n",
        "# # Set the name of the file to be deletedhttps://github.com/iamsrijit/Nepse/blob/a40ff94cbbaed3c2bbd07170c3ed4454d495be79/Nepse%20%20data%202022%20to%202024%20may.xlsx\n",
        "# file_to_delete = 'Nepse data 2022 to 2024 may.xlsx'  # Replace with the file you want to delete\n",
        "\n",
        "# # Remove the file from the repository\n",
        "# file_path = f'{repo_dir}/{file_to_delete}'\n",
        "# repo.index.remove([file_path], working_tree=True)\n",
        "\n",
        "# # Commit the deletion\n",
        "# repo.index.commit(f'Deleted {file_to_delete}')\n",
        "\n",
        "# # Set up the remote repository with authentication\n",
        "# origin = repo.remote(name='origin')\n",
        "# origin.set_url('https://iamsrijit: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@github.com/iamsrijit/Nepse.git')\n",
        "\n",
        "# # Push the changes to GitHub\n",
        "# origin.push()\n",
        "\n",
        "# print(f\"{file_to_delete} has been deleted from GitHub repository.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Seyhv-40MfwG",
        "outputId": "aebe68e0-776d-4631-e531-535758530324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest EMA file: EMA_Cross for_2024-09-13.csv\n",
            "Files to delete for EMA: ['EMA_Cross for_2024-09-12.csv']\n",
            "Deleting file: /content/nepse_new/EMA_Cross for_2024-09-12.csv\n",
            "Deleted EMA_Cross for_2024-09-12.csv\n",
            "Latest espn file: espen_2024-09-13.csv\n",
            "Files to delete for espn: ['espen_2024-09-12.csv']\n",
            "Deleting file: /content/nepse_new/espen_2024-09-12.csv\n",
            "Deleted espen_2024-09-12.csv\n",
            "Pushed changes to GitHub.\n",
            "Previous files have been deleted from GitHub repository.\n"
          ]
        }
      ],
      "source": [
        "from git import Repo\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Function to identify the latest file for a given pattern\n",
        "def get_latest_file(pattern):\n",
        "    files = [f for f in os.listdir(repo_dir) if re.match(pattern, f)]\n",
        "    if files:\n",
        "        latest_file = max(files)\n",
        "        return latest_file\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Replace with your GitHub repository URL\n",
        "repo_url = 'https://github.com/iamsrijit/Nepse.git'\n",
        "repo_dir = '/content/nepse_new'  # Directory to clone the repository\n",
        "\n",
        "# Check if the directory exists and is not empty\n",
        "if os.path.exists(repo_dir) and os.listdir(repo_dir):\n",
        "    repo = Repo(repo_dir)\n",
        "else:\n",
        "    # Clone the repository\n",
        "    repo = Repo.clone_from(repo_url, repo_dir)\n",
        "\n",
        "# Delete older files for EMA_Cross for_\n",
        "latest_ema_file = get_latest_file('^EMA_Cross for_')\n",
        "if latest_ema_file:\n",
        "    print(f\"Latest EMA file: {latest_ema_file}\")\n",
        "    files_to_delete = [f for f in os.listdir(repo_dir) if re.match('^EMA_Cross for_', f) and f != latest_ema_file]\n",
        "    print(f\"Files to delete for EMA: {files_to_delete}\")\n",
        "    for file_to_delete in files_to_delete:\n",
        "        file_path = os.path.join(repo_dir, file_to_delete)\n",
        "        print(f\"Deleting file: {file_path}\")\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "            repo.index.remove([file_path], working_tree=True)\n",
        "            repo.index.commit(f'Deleted {file_to_delete}')\n",
        "            print(f\"Deleted {file_to_delete}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting {file_to_delete}: {e}\")\n",
        "\n",
        "# Delete older files for espen_\n",
        "latest_espen_file = get_latest_file('^espen_')\n",
        "if latest_espen_file:\n",
        "    print(f\"Latest espn file: {latest_espen_file}\")\n",
        "    files_to_delete = [f for f in os.listdir(repo_dir) if re.match('^espen_', f) and f != latest_espen_file]\n",
        "    print(f\"Files to delete for espn: {files_to_delete}\")\n",
        "    for file_to_delete in files_to_delete:\n",
        "        file_path = os.path.join(repo_dir, file_to_delete)\n",
        "        print(f\"Deleting file: {file_path}\")\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "            repo.index.remove([file_path], working_tree=True)\n",
        "            repo.index.commit(f'Deleted {file_to_delete}')\n",
        "            print(f\"Deleted {file_to_delete}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting {file_to_delete}: {e}\")\n",
        "\n",
        "# Set up the remote repository with authentication\n",
        "origin = repo.remote(name='origin')\n",
        "origin.set_url('https://iamsrijit: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@github.com/iamsrijit/Nepse.git')\n",
        "\n",
        "# Push the changes to GitHub\n",
        "try:\n",
        "    origin.push()\n",
        "    print(\"Pushed changes to GitHub.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error pushing changes to GitHub: {e}\")\n",
        "\n",
        "print(\"Previous files have been deleted from GitHub repository.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cBAOm7ZLTa1p"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}