# -*- coding: utf-8 -*-
"""3_vs_9

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127xtgMNfbzuehAj4qBf3inyqlESZ4Jz0
"""

# 3_vs_9.py
# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import re
import base64
from datetime import datetime, timedelta
import os

# ===============================
# STEP 1: Get latest CSV from repo
# ===============================
def get_latest_file_url(repo_url):
    response = requests.get(repo_url)
    soup = BeautifulSoup(response.content, 'html.parser')
    file_links = soup.find_all('a', href=True)

    file_urls = {}
    for link in file_links:
        file_name = link['href']
        if file_name.endswith('.csv'):
            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', file_name)
            if date_match:
                file_date = date_match.group(1)
                file_urls[file_date] = repo_url.replace('/tree/', '/raw/') + '/' + file_name

    if not file_urls:
        raise ValueError("No CSV files found in the repository.")

    latest_file_date = max(file_urls.keys())
    return file_urls[latest_file_date]

repo_url = 'https://github.com/iamsrijit/Nepse/tree/main'
latest_file_url = get_latest_file_url(repo_url)
latest_file_url = latest_file_url.replace('/iamsrijit/Nepse/blob/main/', '/')
secondss = pd.read_csv(latest_file_url)

# ===============================
# STEP 2: Clean & preprocess data
# ===============================
# Define expected columns
expected_columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'Volume']

# Optional sanity check
missing_cols = set(expected_columns) - set(secondss.columns)
if missing_cols:
    raise ValueError(f"The following expected columns are missing from the CSV: {missing_cols}")

# Keep only necessary columns
secondss = secondss[expected_columns]

dfs = [secondss]
finall_df = pd.DataFrame()

for df in dfs:
    if 'Date' not in df.columns:
        continue
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df.dropna(subset=['Date'], inplace=True)

combined_df = pd.concat(dfs, ignore_index=True, join='outer')
combined_df['Date'] = pd.to_datetime(combined_df['Date'])

# Remove duplicates per Symbol, Date
for symbol in combined_df['Symbol'].unique():
    symbol_df = combined_df[combined_df['Symbol'] == symbol]
    symbol_df = symbol_df.sort_values(by=['Date'], ascending=False)
    symbol_df = symbol_df.drop_duplicates(subset=['Date'], keep='first')
    finall_df = pd.concat([finall_df, symbol_df], ignore_index=True)

# ===============================
# STEP 3: EMA crossover strategy
# ===============================
finall_df = finall_df.sort_values(by=['Symbol', 'Date'], ascending=[True, True])

results = []
insufficient_data = []

def calculate_ema(prices, span):
    ema = np.full(len(prices), np.nan)
    if len(prices) < span:
        return ema
    ema[span-1] = sum(prices[:span]) / span
    multiplier = 2 / (span + 1)
    for i in range(span, len(prices)):
        ema[i] = (prices[i] - ema[i-1]) * multiplier + ema[i-1]
    return ema

for symbol in finall_df['Symbol'].unique():
    symbol_df = finall_df[finall_df['Symbol'] == symbol].reset_index(drop=True)
    prices = symbol_df['Close'].tolist()

    if len(prices) < 9:
        insufficient_data.append(symbol)
        continue

    ema3 = calculate_ema(prices, 3)
    ema9 = calculate_ema(prices, 9)

    for i in range(1, len(symbol_df)):
        if not np.isnan(ema9[i]) and not np.isnan(ema3[i]) and not np.isnan(ema9[i-1]) and not np.isnan(ema3[i-1]):
            if ema3[i] > ema9[i] and ema3[i-1] < ema9[i-1]:
                results.append([symbol, symbol_df['Date'][i], "Buy", symbol_df['Close'][i]])
            elif ema3[i] < ema9[i] and ema3[i-1] > ema9[i-1]:
                results.append([symbol, symbol_df['Date'][i], "Sell", symbol_df['Close'][i]])

cross_signals_df = pd.DataFrame(results, columns=['Symbol', 'Date', 'Signal', 'Close'])

# Filter last 300 days
three_hundred_days_ago = datetime.today() - timedelta(days=300)
cross_signals_df['Date'] = pd.to_datetime(cross_signals_df['Date'])
filtered_df = cross_signals_df[cross_signals_df['Date'] >= three_hundred_days_ago]
filtered_df = filtered_df.sort_values(by='Date', ascending=False)

# ===============================
# STEP 4: Upload new file & delete old ones
# ===============================
try:
    csv_data = filtered_df.to_csv(index=False)
    csv_data_base64 = base64.b64encode(csv_data.encode()).decode()

    repo = "iamsrijit/Nepse"
    file_name = f'3vs9EMA_Cross_for_{datetime.today().strftime("%Y-%m-%d")}.csv'
    file_path = file_name
    token = os.environ.get("GH_TOKEN")

    if not token:
        raise ValueError("No GH_TOKEN found. Did you set secrets in GitHub Actions?")

    headers = {
        'Authorization': f'token {token}',
        'Accept': 'application/vnd.github.v3+json'
    }

    # List all files in the repository
    contents_url = f'https://api.github.com/repos/{repo}/contents'
    contents_response = requests.get(contents_url, headers=headers)

    if contents_response.status_code != 200:
        raise Exception("Failed to fetch repository contents.")

    contents = contents_response.json()

    # Delete old files starting with '3vs9EMA_Cross_for_' except today's file
    for item in contents:
        if item['name'].startswith('3vs9EMA_Cross_for_') and item['name'] != file_name:
            delete_url = f"https://api.github.com/repos/{repo}/contents/{item['name']}"
            delete_payload = {
                "message": f"Delete old file {item['name']}",
                "sha": item['sha'],
                "branch": "main"
            }
            delete_response = requests.delete(delete_url, headers=headers, json=delete_payload)
            if delete_response.status_code == 200:
                print(f'üóëÔ∏è Deleted old file: {item["name"]}')
            else:
                print(f'‚ö†Ô∏è Failed to delete {item["name"]}. Status: {delete_response.status_code}')

    # Upload the new file
    upload_url = f'https://api.github.com/repos/{repo}/contents/{file_path}'
    check_response = requests.get(upload_url, headers=headers)
    sha = check_response.json()['sha'] if check_response.status_code == 200 else None

    payload = {
        'message': f'Upload {file_name}',
        'content': csv_data_base64,
        'branch': 'main'
    }
    if sha:
        payload['sha'] = sha

    upload_response = requests.put(upload_url, headers=headers, json=payload)

    if upload_response.status_code in [200, 201]:
        print(f'‚úÖ File {file_name} uploaded successfully!')
    else:
        print(f'‚ùå Failed to upload {file_name}. Status: {upload_response.status_code}')
        print('Response:', upload_response.json())

except Exception as e:
    print("An error occurred:", e)