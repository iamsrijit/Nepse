# -*- coding: utf-8 -*-
"""nepse_automation

Automatically generated by Colab.

Original file is located at

"""

# import os
# import pandas as pd
# from nepse_scraper import Nepse_scraper
# from datetime import datetime
# import requests
# from bs4 import BeautifulSoup
# import re
# import base64
# import numpy as np
# from git import Repo

# -*- coding: utf-8 -*-
"""nepse_automation

Automatically generated by Colab.
"""

import os
import pandas as pd
from nepse_scraper import Nepse_scraper
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import re
import base64
import numpy as np
from git import Repo

# ----------------------------
# Daily Data Scraping Section
# ----------------------------
def get_daily_data():
    request_obj = Nepse_scraper()
    today_price = request_obj.get_today_price()
    content_data = today_price.get('content', [])

    filtered_data = []
    columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'Volume']

    for item in content_data:
        symbol = item.get('symbol', '')
        date = item.get('businessDate', '')
        open_price = item.get('openPrice', 0)
        high_price = item.get('highPrice', 0)
        low_price = item.get('lowPrice', 0)
        close_price = item.get('closePrice', 0)
        volume_daily = item.get('totalTradedValue', 0)
        percent_change = ((close_price - open_price) / open_price * 100) if open_price else 0

        filtered_data.append({
            'Symbol': symbol,
            'Date': date,
            'Open': open_price,
            'High': high_price,
            'Low': low_price,
            'Close': close_price,
            'Percent Change': round(percent_change, 2),
            'Volume': volume_daily
        })

    return pd.DataFrame(filtered_data)

# ... [Rest of the code remains unchanged] ...
# Install required dependencies
# subprocess.run(["pip", "install", "nepse-scraper", "xlsxwriter", "gitpython", "pandas", "requests", "beautifulsoup4", "joblib", "matplotlib"], check=True)

# ----------------------------
# Daily Data Scraping Section
# ----------------------------
# def get_daily_data():
#     request_obj = Nepse_scraper()
#     today_price = request_obj.get_today_price()
#     content_data = today_price.get('content', [])

#     filtered_data = []
#     columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'Volume']

#     for item in content_data:
#         symbol = item.get('symbol', '')
#         date = item.get('businessDate', '')
#         open_price = item.get('openPrice', 0)
#         high_price = item.get('highPrice', 0)
#         low_price = item.get('lowPrice', 0)
#         close_price = item.get('closePrice', 0)
#         volume_daily = item.get('totalTradedValue', 0)
#         percent_change = ((close_price - open_price) / open_price * 100) if open_price else 0

#         filtered_data.append({
#             'Symbol': symbol,
#             'Date': date,
#             'Open': open_price,
#             'High': high_price,
#             'Low': low_price,
#             'Close': close_price,
#             'Percent Change': round(percent_change, 2),
#             'Volume': volume_daily
#         })

#     return pd.DataFrame(filtered_data)

# ----------------------------
# GitHub Data Handling Section
# ----------------------------
def github_file_operations():
    try:
        token = os.getenv("GH_TOKEN")
        headers = {"Authorization": f"token {token}"}
        api_url = "https://api.github.com/repos/iamsrijit/Nepse/contents/"

        response = requests.get(api_url, headers=headers)
        files = [f for f in response.json() if f['name'].startswith('espen_')]

        if files:
            latest_file = max(files, key=lambda x: x['name'])
            download_url = latest_file['download_url']
            return pd.read_csv(download_url)

    except Exception as e:
        print(f"GitHub file error: {str(e)}")

    return pd.DataFrame()

# ----------------------------
# Data Processing Section
# ----------------------------
def process_data(first, secondss):
    exclude_symbols = [
        'SAEF', 'SEF', 'CMF1', 'NICGF', 'NBF2', 'CMF2', 'NMB50', 'SIGS2', 'NICBF',
        'SFMF', 'LUK', 'SLCF', 'KEF', 'SBCF', 'PSF', 'NIBSF2', 'NICSF', 'RMF1',
        'NBF3', 'MMF1', 'KDBY', 'NICFC', 'GIBF1', 'NSIF2', 'SAGF', 'NIBLGF',
        'SFEF', 'PRSF', 'C30MF', 'SIGS3', 'RMF2', 'LVF2', 'H8020', 'NICGF2',
        'NIBLSTF', 'KSY', 'NBLD87', 'PBD88', 'OTHERS','HIDCLP','NIMBPO','MUTUAL',
        'CIT','ILI','LEMF','NIBLPF','INVESTMENT','SENFLOAT','HEIP' ,'SBID83','NICAD8283'
    ]

    combined = pd.concat([first, secondss])
    combined = combined[~combined['Symbol'].isin(exclude_symbols)]
    combined['Date'] = pd.to_datetime(combined['Date']).dt.strftime('%Y-%m-%d')

    return combined.sort_values(['Symbol', 'Date'], ascending=[True, False]) \
                  .drop_duplicates(['Symbol', 'Date'])

# ----------------------------
# GitHub Upload Section
# ----------------------------
def github_upload(df, file_prefix):
    try:
        token = os.getenv("GH_TOKEN")
        csv_data = df.to_csv(index=False)
        csv_b64 = base64.b64encode(csv_data.encode()).decode()

        file_name = f"{file_prefix}_{datetime.today().strftime('%Y-%m-%d')}.csv"
        api_url = f"https://api.github.com/repos/iamsrijit/Nepse/contents/{file_name}"

        headers = {
            "Authorization": f"token {token}",
            "Accept": "application/vnd.github.v3+json"
        }

        # Check existing file
        response = requests.get(api_url, headers=headers)
        sha = response.json().get('sha') if response.status_code == 200 else None

        payload = {
            "message": f"Auto-update {file_name}",
            "content": csv_b64,
            "branch": "main"
        }
        if sha:
            payload["sha"] = sha

        response = requests.put(api_url, headers=headers, json=payload)
        response.raise_for_status()
        print(f"Successfully uploaded {file_name}")

    except Exception as e:
        print(f"Upload error: {str(e)}")

# ----------------------------
# Technical Analysis Section
# ----------------------------
def technical_analysis(df):
    df['Date'] = pd.to_datetime(df['Date'])
    df = df.sort_values(['Symbol', 'Date'])

    # Calculate indicators
    df['EMA_20'] = df.groupby('Symbol')['Close'].transform(lambda x: x.ewm(span=20).mean())
    df['EMA_50'] = df.groupby('Symbol')['Close'].transform(lambda x: x.ewm(span=50).mean())

    # RSI calculation
    delta = df.groupby('Symbol')['Close'].diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.groupby(df['Symbol']).transform(lambda x: x.rolling(14).mean())
    avg_loss = loss.groupby(df['Symbol']).transform(lambda x: x.rolling(14).mean())
    rs = avg_gain / avg_loss
    df['RSI'] = 100 - (100 / (1 + rs))

    # Crossover detection
    df['Crossover'] = (df['EMA_20'] > df['EMA_50']) & (df['EMA_20'].shift() <= df['EMA_50'].shift())

    return df[df['Crossover']].sort_values('Date', ascending=False) \
                              .drop_duplicates('Symbol')

# ----------------------------
# Main Execution Flow
# ----------------------------
if __name__ == "__main__":
    # Step 1: Get daily data
    daily_data = get_daily_data()

    # Step 2: Get historical data
    historical_data = github_file_operations()

    if not historical_data.empty:
        # Step 3: Process and merge data
        merged_data = process_data(daily_data, historical_data)
        github_upload(merged_data, "espen")

        # Step 4: Technical analysis
        ema_results = technical_analysis(merged_data)
        print("\nLatest EMA Crossovers:")
        print(ema_results[['Symbol', 'Date', 'Close']])
        github_upload(ema_results, "EMA_Cross_for")

    print("Process completed successfully!")
