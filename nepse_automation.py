# -*- coding: utf-8 -*-
"""nepse_automation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kNp9iA-Cs3MYVnvvUXEoD5p0V6DDT5A-
"""

# -*- coding: utf-8 -*-
"""nepse_automation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HROyRxfQR9BdshPq56d9Xo9XWLnfpi6g
"""

import subprocess
import pandas as pd
from nepse_scraper import Nepse_scraper
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import re
import os
import base64
import numpy as np
from joblib import Parallel, delayed
from git import Repo

# Install required dependencies
subprocess.run(["pip", "install", "nepse-scraper", "xlsxwriter", "gitpython", "pandas", "requests", "beautifulsoup4", "joblib", "matplotlib"], check=True)

# ----------------------------
# Daily Data Scraping Section
# ----------------------------
def get_daily_data():
    request_obj = Nepse_scraper()
    today_price = request_obj.get_today_price()
    content_data = today_price.get('content', [])

    filtered_data = []
    columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'Volume']

    for item in content_data:
        symbol = item.get('symbol', '')
        date = item.get('businessDate', '')
        open_price = item.get('openPrice', 0)
        high_price = item.get('highPrice', 0)
        low_price = item.get('lowPrice', 0)
        close_price = item.get('closePrice', 0)
        volume_daily = item.get('totalTradedValue', 0)
        percent_change = ((close_price - open_price) / open_price * 100) if open_price else 0

        filtered_data.append({
            'Symbol': symbol,
            'Date': date,
            'Open': open_price,
            'High': high_price,
            'Low': low_price,
            'Close': close_price,
            'Percent Change': round(percent_change, 2),
            'Volume': volume_daily
        })

    return pd.DataFrame(filtered_data)

# ----------------------------
# GitHub Data Handling Section
# ----------------------------
def github_file_operations():
    # Get latest historical data
    repo_url = 'https://github.com/iamsrijit/Nepse'
    response = requests.get(f"{repo_url}/tree/main")
    soup = BeautifulSoup(response.content, 'html.parser')

    file_urls = {}
    for link in soup.find_all('a', href=True):
        href = link['href']
        if '/iamsrijit/Nepse/blob/main/espen_' in href:
            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', href)
            if date_match:
                file_date = date_match.group(1)
                raw_url = href.replace('/blob/', '/raw/')
                file_urls[file_date] = f"https://github.com{raw_url}"

    if file_urls:
        latest_date = max(file_urls.keys())
        latest_url = file_urls[latest_date]
        return pd.read_csv(latest_url)
    return pd.DataFrame()

# ----------------------------
# Data Processing Section
# ----------------------------
def process_data(first, secondss):
    exclude_symbols = [
        'SAEF', 'SEF', 'CMF1', 'NICGF', 'NBF2', 'CMF2', 'NMB50', 'SIGS2', 'NICBF',
        'SFMF', 'LUK', 'SLCF', 'KEF', 'SBCF', 'PSF', 'NIBSF2', 'NICSF', 'RMF1',
        'NBF3', 'MMF1', 'KDBY', 'NICFC', 'GIBF1', 'NSIF2', 'SAGF', 'NIBLGF',
        'SFEF', 'PRSF', 'C30MF', 'SIGS3', 'RMF2', 'LVF2', 'H8020', 'NICGF2',
        'NIBLSTF', 'KSY', 'NBLD87', 'PBD88', 'OTHERS','HIDCLP','NIMBPO','MUTUAL',
        'CIT','ILI','LEMF','NIBLPF','INVESTMENT','SENFLOAT','HEIP' ,'SBID83','NICAD8283'
    ]

    combined = pd.concat([first, secondss])
    combined = combined[~combined['Symbol'].isin(exclude_symbols)]
    combined['Date'] = pd.to_datetime(combined['Date']).dt.strftime('%Y-%m-%d')

    finall_df = pd.DataFrame()
    for symbol in combined['Symbol'].unique():
        symbol_df = combined[combined['Symbol'] == symbol]
        symbol_df = symbol_df.sort_values('Date', ascending=False)
        symbol_df = symbol_df.drop_duplicates('Date', keep='first')
        finall_df = pd.concat([finall_df, symbol_df])

    return finall_df

# ----------------------------
# GitHub Upload Section
# ----------------------------
def github_upload(df, file_prefix):
    try:
        token = os.getenv("GH_TOKEN")
        csv_data = df.to_csv(index=False)
        csv_b64 = base64.b64encode(csv_data.encode()).decode()

        file_name = f"{file_prefix}_{datetime.today().strftime('%Y-%m-%d')}.csv"
        upload_url = f"https://api.github.com/repos/iamsrijit/Nepse/contents/{file_name}"

        headers = {
            "Authorization": f"token {token}",
            "Accept": "application/vnd.github.v3+json"
        }

        # Check if file exists
        existing = requests.get(upload_url, headers=headers)
        sha = existing.json().get('sha', None) if existing.status_code == 200 else None

        payload = {
            "message": f"Auto-update {file_name}",
            "content": csv_b64,
            "branch": "main"
        }
        if sha:
            payload["sha"] = sha

        response = requests.put(upload_url, headers=headers, json=payload)

        if response.status_code in [200, 201]:
            print(f"Successfully uploaded {file_name}")
        else:
            print(f"Failed to upload {file_name}. Status: {response.status_code}")
            print("Response:", response.json())

    except Exception as e:
        print(f"Upload error: {str(e)}")

# ----------------------------
# Technical Analysis Section
# ----------------------------
def technical_analysis(finall_df):
    finall_df['Date'] = pd.to_datetime(finall_df['Date'])
    finall_df = finall_df.sort_values(['Symbol', 'Date'])

    # Calculate indicators
    finall_df['EMA_20'] = finall_df.groupby('Symbol')['Close'].transform(lambda x: x.ewm(span=20, adjust=False).mean())
    finall_df['EMA_50'] = finall_df.groupby('Symbol')['Close'].transform(lambda x: x.ewm(span=50, adjust=False).mean())

    # RSI calculation
    delta = finall_df.groupby('Symbol')['Close'].diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.groupby(finall_df['Symbol']).transform(lambda x: x.rolling(14).mean())
    avg_loss = loss.groupby(finall_df['Symbol']).transform(lambda x: x.rolling(14).mean())
    rs = avg_gain / avg_loss
    finall_df['RSI'] = 100 - (100 / (1 + rs))

    # Crossover detection
    finall_df['Crossover'] = (finall_df['EMA_20'] > finall_df['EMA_50']) & (finall_df['EMA_20'].shift(1) <= finall_df['EMA_50'].shift(1))
    valid_crossovers = finall_df[
        finall_df['Crossover'] &
        (finall_df['RSI'].between(30, 70)) &
        (finall_df['Close'] > finall_df.groupby('Symbol')['Close'].transform(lambda x: x.rolling(60).max().shift(1)) * 0.95)
    ]

    return valid_crossovers.sort_values('Date', ascending=False).drop_duplicates('Symbol')

# ----------------------------
# Main Execution Flow
# ----------------------------
if __name__ == "__main__":
    # Step 1: Get daily data
    first = get_daily_data()

    # Step 2: Get historical data
    secondss = github_file_operations()

    # Step 3: Process and merge data
    if not secondss.empty:
        finall_df = process_data(first, secondss)
        # Upload daily data
        github_upload(finall_df, "espen")

        # Step 4: Technical analysis
        ema_results = technical_analysis(finall_df)
        print("\nLatest EMA Crossovers:")
        print(ema_results[['Symbol', 'Date', 'Close']])

        # Upload EMA results
        github_upload(ema_results, "EMA_Cross_for")

    # Step 5: Cleanup old files
    try:
        repo_dir = "/tmp/nepse_repo"
        Repo.clone_from("https://github.com/iamsrijit/Nepse.git", repo_dir)
        repo = Repo(repo_dir)

        # Delete old files (keep last 3)
        all_files = os.listdir(repo_dir)
        patterns = [r'^espen_\d{4}-\d{2}-\d{2}\.csv$', r'^EMA_Cross_for_\d{4}-\d{2}-\d{2}\.csv$']

        for pattern in patterns:
            matches = sorted([f for f in all_files if re.match(pattern, f)], reverse=True)[3:]
            for file in matches:
                os.remove(os.path.join(repo_dir, file))
                repo.index.remove([file], working_tree=True)

        if repo.is_dirty():
            repo.git.add(A=True)
            repo.index.commit("Auto-cleanup old files")
            origin = repo.remote(name='origin')
            origin.push()

    except Exception as e:
        print(f"Cleanup error: {str(e)}")

    print("Process completed successfully!")