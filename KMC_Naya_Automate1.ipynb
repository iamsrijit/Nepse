{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EfjJB0z6MDiQ",
        "outputId": "badd975e-7a97-4ef7-d3b5-d9e8ae59f371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nepse-scraper\n",
            "  Downloading nepse_scraper-0.1.7.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from nepse-scraper) (2.32.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from nepse-scraper) (2.0.7)\n",
            "Collecting wasmtime (from nepse-scraper)\n",
            "  Downloading wasmtime-24.0.0-py3-none-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting retrying (from nepse-scraper)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->nepse-scraper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->nepse-scraper) (3.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->nepse-scraper) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->nepse-scraper) (1.16.0)\n",
            "Requirement already satisfied: importlib-resources>=5.10 in /usr/local/lib/python3.10/dist-packages (from wasmtime->nepse-scraper) (6.4.5)\n",
            "Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading wasmtime-24.0.0-py3-none-manylinux1_x86_64.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nepse-scraper\n",
            "  Building wheel for nepse-scraper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nepse-scraper: filename=nepse_scraper-0.1.7-py3-none-any.whl size=10151 sha256=fcb5cba56615f954140b61997732250d172242b39470f8cab387021571bdd063\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/f8/66/819db2d7e2ecbdd45cf61624eebb79f362c758750202721aaf\n",
            "Successfully built nepse-scraper\n",
            "Installing collected packages: wasmtime, retrying, nepse-scraper\n",
            "Successfully installed nepse-scraper-0.1.7 retrying-1.3.4 wasmtime-24.0.0\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 smmap-5.0.1\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.43)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.11)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nepse-scraper\n",
        "!pip install xlsxwriter\n",
        "!pip install gitpython\n",
        "!pip install gitpython pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pol02ZhnAk6f"
      },
      "source": [
        "**Daily Nepse Scrapping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InYE7C5V5ZZc"
      },
      "outputs": [],
      "source": [
        "from nepse_scraper import Nepse_scraper\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create an object from the Nepse_scraper class\n",
        "request_obj = Nepse_scraper()\n",
        "\n",
        "# Get today's price from NEPSE\n",
        "today_price = request_obj.get_today_price()\n",
        "\n",
        "# Extract the 'content' section from the response\n",
        "content_data = today_price.get('content', [])\n",
        "\n",
        "# Initialize an empty list to store filtered data\n",
        "filtered_data = []\n",
        "\n",
        "# Define the column names\n",
        "columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close','Volume']\n",
        "\n",
        "# Iterate over each item in the 'content' section\n",
        "for item in content_data:\n",
        "    symbol = item.get('symbol', '')\n",
        "    date = item.get('businessDate', '')\n",
        "    open_price = item.get('openPrice', '')\n",
        "    high_price = item.get('highPrice', '')\n",
        "    low_price = item.get('lowPrice', '')\n",
        "    close_price = item.get('closePrice', '')\n",
        "    Volume_daily = item.get('totalTradedValue', '')\n",
        "\n",
        "    # Append the extracted values to the filtered data list\n",
        "    filtered_data.append({\n",
        "        'Symbol': symbol,\n",
        "        'Date': date,\n",
        "        'Open': open_price,\n",
        "        'High': high_price,\n",
        "        'Low': low_price,\n",
        "        'Close': close_price,\n",
        "        'Volume': Volume_daily\n",
        "    })\n",
        "\n",
        "# Create DataFrame from filtered data\n",
        "first = pd.DataFrame(filtered_data)\n",
        "\n",
        "# Check if DataFrame has data\n",
        "if not first.empty:\n",
        "    # Display DataFrame\n",
        "    print(first)\n",
        "\n",
        "    # Get today's day name\n",
        "    today_day_name = datetime.now().strftime('%A')\n",
        "\n",
        "    # Save DataFrame to CSV with today's day name in the filename\n",
        "    file_name = f'nepse_{today_day_name}.csv'\n",
        "    first.to_csv(file_name, index=False)\n",
        "\n",
        "    print(f\"Data saved to '{file_name}'\")\n",
        "else:\n",
        "    print(\"No data available to create DataFrame.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqyPMMBgAuV7"
      },
      "source": [
        "**access nepse data till July 31**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Fz5o4-whJmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Function to get the latest file URL\n",
        "def get_latest_file_url(repo_url):\n",
        "    # Send a GET request to the GitHub repository\n",
        "    response = requests.get(repo_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all links to files in the repository\n",
        "    file_links = soup.find_all('a', href=True)\n",
        "\n",
        "    # Extract file names and corresponding URLs\n",
        "    file_urls = {}\n",
        "    for link in file_links:\n",
        "        file_name = link['href']\n",
        "        if file_name.endswith('.csv'):\n",
        "            date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', file_name)\n",
        "            if date_match:\n",
        "                file_date = date_match.group(1)\n",
        "                file_urls[file_date] = repo_url.replace('/tree/', '/raw/') + '/' + file_name\n",
        "\n",
        "    if not file_urls:\n",
        "        raise ValueError(\"No CSV files found in the repository.\")\n",
        "\n",
        "    # Get the latest file URL based on the date\n",
        "    latest_file_date = max(file_urls.keys())\n",
        "    latest_file_url = file_urls[latest_file_date]\n",
        "    print(\"Latest file date:\", latest_file_date)\n",
        "    print(\"Latest file URL:\", latest_file_url)\n",
        "    return latest_file_url\n",
        "\n",
        "# Replace with the actual GitHub repository URL\n",
        "repo_url = 'https://github.com/iamsrijit/Nepse/tree/main'\n",
        "\n",
        "try:\n",
        "    # Get the latest file URL\n",
        "    latest_file_url = get_latest_file_url(repo_url)\n",
        "\n",
        "    # Correct the file URL\n",
        "    latest_file_url = latest_file_url.replace('/iamsrijit/Nepse/blob/main/', '/')\n",
        "\n",
        "    # Read data from the latest file\n",
        "    secondss = pd.read_csv(latest_file_url)\n",
        "\n",
        "    # Assuming the column names are the same as in your example\n",
        "    secondss.columns = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close','Volume']\n",
        "\n",
        "    print(secondss.head())\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR6bYdIJ8C-j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tn06vxvyhFt"
      },
      "source": [
        "**merging ma date anusar milaunu paryoo**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HROkozh3qNxZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'first' and 'secondss' are your DataFrames\n",
        "dfs = [first, secondss]\n",
        "\n",
        "# Create an empty DataFrame to store the final results\n",
        "final_df = pd.DataFrame()\n",
        "\n",
        "for df in dfs:\n",
        "    try:\n",
        "        if 'Date' not in df.columns:\n",
        "            print(f\"'Date' column not found in DataFrame.\")\n",
        "            continue\n",
        "\n",
        "        # Convert the 'Date' column to datetime format and drop rows with invalid dates\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%m/%d/%Y')\n",
        "        df.dropna(subset=['Date'], inplace=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing DataFrame: {e}\")\n",
        "\n",
        "# Combine all the DataFrames\n",
        "if not dfs:\n",
        "    print(\"No valid data to process.\")\n",
        "else:\n",
        "    combined_df = pd.concat(dfs, ignore_index=True, join='outer')\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], format='%m/%d/%Y')\n",
        "\n",
        "    # Iterate over each unique symbol\n",
        "    for symbol in combined_df['Symbol'].unique():\n",
        "        symbol_df = combined_df[combined_df['Symbol'] == symbol]\n",
        "        symbol_df = symbol_df.sort_values(by=['Date'], ascending=False)\n",
        "        symbol_df = symbol_df.drop_duplicates(subset=['Date'], keep='first')\n",
        "\n",
        "        # Initialize columns 'G' and 'H' as None\n",
        "        symbol_df['G'] = None\n",
        "        symbol_df['H'] = None\n",
        "\n",
        "        # Append to the final DataFrame\n",
        "        final_df = pd.concat([final_df, symbol_df], ignore_index=True)\n",
        "\n",
        "    # Save the final DataFrame to a CSV file\n",
        "    output_file_name = 'combined_data.csv'\n",
        "\n",
        "    # Write the headers to the CSV file\n",
        "    with open(output_file_name, 'w') as f:\n",
        "        headers = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'G', 'H','Volume']\n",
        "        f.write(','.join(headers) + '\\n')\n",
        "\n",
        "    # Append the final DataFrame without headers to the CSV file\n",
        "    final_df.to_csv(output_file_name, mode='a', index=False, header=False)\n",
        "\n",
        "    # Optional: print the first few rows of the final DataFrame\n",
        "    # print(final_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w-oZgM_s8x--"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJHZm2pa80_s"
      },
      "source": [
        "**exclude mutual funds **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f7vVS9Ho80ce"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'first' and 'secondss' are your DataFrames\n",
        "dfs = [first, secondss]\n",
        "\n",
        "# Create an empty DataFrame to store the final results\n",
        "final_df = pd.DataFrame()\n",
        "\n",
        "# List of symbols to exclude\n",
        "exclude_symbols = [\n",
        "    'SAEF', 'SEF', 'CMF1', 'NICGF', 'NBF2', 'CMF2', 'NMB50', 'SIGS2', 'NICBF',\n",
        "    'SFMF', 'LUK', 'SLCF', 'KEF', 'SBCF', 'PSF', 'NIBSF2', 'NICSF', 'RMF1',\n",
        "    'NBF3', 'MMF1', 'KDBY', 'NICFC', 'GIBF1', 'NSIF2', 'SAGF', 'NIBLGF',\n",
        "    'SFEF', 'PRSF', 'C30MF', 'SIGS3', 'RMF2', 'LVF2', 'H8020', 'NICGF2',\n",
        "    'NIBLSTF', 'KSY' ,'NBLD87', 'PBD88', 'OTHERS','HIDCLP','NIMBPO','MUTUAL',\n",
        "    'CIT','ILI','LEMF','NIBLPF','INVESTMENT','SENFLOAT','HEIP' ,'SBID83','NICAD8283'\n",
        "]\n",
        "\n",
        "for df in dfs:\n",
        "    try:\n",
        "        if 'Date' not in df.columns:\n",
        "            print(f\"'Date' column not found in DataFrame.\")\n",
        "            continue\n",
        "\n",
        "        # Convert the 'Date' column to datetime format and drop rows with invalid dates\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce').dt.strftime('%m/%d/%Y')\n",
        "        df.dropna(subset=['Date'], inplace=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing DataFrame: {e}\")\n",
        "\n",
        "# Combine all the DataFrames\n",
        "if not dfs:\n",
        "    print(\"No valid data to process.\")\n",
        "else:\n",
        "    combined_df = pd.concat(dfs, ignore_index=True, join='outer')\n",
        "\n",
        "    # Filter out the excluded symbols\n",
        "    combined_df = combined_df[~combined_df['Symbol'].isin(exclude_symbols)]\n",
        "\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], format='%m/%d/%Y')\n",
        "\n",
        "    # Iterate over each unique symbol\n",
        "    for symbol in combined_df['Symbol'].unique():\n",
        "        symbol_df = combined_df[combined_df['Symbol'] == symbol]\n",
        "        symbol_df = symbol_df.sort_values(by=['Date'], ascending=False)\n",
        "        symbol_df = symbol_df.drop_duplicates(subset=['Date'], keep='first')\n",
        "\n",
        "        # Initialize columns 'G' and 'H' as None\n",
        "        symbol_df['G'] = None\n",
        "        symbol_df['H'] = None\n",
        "\n",
        "        # Append to the final DataFrame\n",
        "        final_df = pd.concat([final_df, symbol_df], ignore_index=True)\n",
        "\n",
        "    # Save the final DataFrame to a CSV file\n",
        "    output_file_name = 'combined_data.csv'\n",
        "\n",
        "    # Write the headers to the CSV file\n",
        "    with open(output_file_name, 'w') as f:\n",
        "        headers = ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Percent Change', 'G', 'H','Volume']\n",
        "        f.write(','.join(headers) + '\\n')\n",
        "\n",
        "    # Append the final DataFrame without headers to the CSV file\n",
        "    final_df.to_csv(output_file_name, mode='a', index=False, header=False)\n",
        "\n",
        "    # Optional: print the first few rows of the final DataFrame\n",
        "    # print(final_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iAE05Vp9cTQ"
      },
      "source": [
        "**THIS CODE NEED TO BE TESTED FOR LARGE DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qcYe8ZrFcSqO"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "import base64\n",
        "\n",
        "# Assuming you have sorted_df DataFrame containing your data\n",
        "# Assuming sorted_df is defined elsewhere in your code\n",
        "\n",
        "try:\n",
        "    # Convert sorted_df to CSV format\n",
        "    csv_data = final_df.to_csv(index=False)\n",
        "\n",
        "    # Encode the CSV data to Base64\n",
        "    csv_data_base64 = base64.b64encode(csv_data.encode()).decode()\n",
        "\n",
        "    # Define the GitHub repository URL\n",
        "    repo_url = 'https://github.com/iamsrijit/Nepse'\n",
        "\n",
        "    # Define the file name with today's date\n",
        "    file_name = f'espen_{datetime.today().strftime(\"%Y-%m-%d\")}.csv'\n",
        "\n",
        "    # Define your personal access token\n",
        "    token = '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'\n",
        "\n",
        "    # Define the file path in the repository\n",
        "    file_path = f'/{file_name}'\n",
        "\n",
        "    # Define the API URL for uploading files to GitHub\n",
        "    upload_url = f'https://api.github.com/repos/iamsrijit/Nepse/contents{file_path}'\n",
        "\n",
        "    # Prepare the headers with the authorization token\n",
        "    headers = {\n",
        "        'Authorization': f'token {token}',\n",
        "        'Accept': 'application/vnd.github.v3+json'\n",
        "    }\n",
        "\n",
        "    # Prepare the payload with file content\n",
        "    payload = {\n",
        "        'message': f'Upload {file_name}',\n",
        "        'content': csv_data_base64,\n",
        "        'branch': 'main'  # Specify the branch you want to upload to\n",
        "    }\n",
        "\n",
        "    # Send a PUT request to upload the file\n",
        "    response = requests.put(upload_url, headers=headers, json=payload)\n",
        "\n",
        "    # Check the response status\n",
        "    if response.status_code == 200:\n",
        "        print(f'File {file_name} uploaded successfully!')\n",
        "    elif response.status_code == 422:\n",
        "        print(f'Failed to upload {file_name}. Status code: 422 Unprocessable Entity')\n",
        "        print('Error Message:', response.json()['message'])\n",
        "    else:\n",
        "        print(f'Failed to upload {file_name}. Status code: {response.status_code}')\n",
        "        print('Response Content:', response.text)\n",
        "\n",
        "except Exception as e:\n",
        "    print('An error occurred:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9_EesUcE2S8"
      },
      "source": [
        "**Status code: 201 is successful**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeeGAnZLMzJi"
      },
      "source": [
        "**bold8/1/2024 text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MIELwNMGzT-5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9I_o8qGwzUkz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn6IQlZ_jHM6"
      },
      "source": [
        "**WITH RSI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RQ384XbRDHBZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Sample DataFrame (final_df) should have columns ['Symbol', 'Date', 'Close']\n",
        "# Make sure final_df is sorted by 'Symbol' and 'Date' (in ascending order)\n",
        "final_df = final_df.sort_values(by=['Symbol', 'Date'], ascending=[True, True])\n",
        "\n",
        "# Check if the number of unique symbols is less than 50\n",
        "if final_df['Symbol'].nunique() < 50:\n",
        "    print(\"The number of unique symbols is less than 50. Please provide more data.\")\n",
        "else:\n",
        "    # Initialize an empty list to store results for each symbol\n",
        "    results = []\n",
        "    insufficient_data = []\n",
        "\n",
        "    # Function to calculate EMA\n",
        "    def calculate_ema(prices, span):\n",
        "        ema = np.full(len(prices), np.nan)\n",
        "        if len(prices) < span:\n",
        "            return ema\n",
        "        ema[span-1] = sum(prices[:span]) / span\n",
        "        multiplier = 2 / (span + 1)\n",
        "        for i in range(span, len(prices)):\n",
        "            ema[i] = (prices[i] - ema[i-1]) * multiplier + ema[i-1]\n",
        "        return ema\n",
        "\n",
        "    # Function to calculate RSI\n",
        "    def calculate_rsi(prices, period=14):\n",
        "        deltas = np.diff(prices)\n",
        "        seed = deltas[:period+1]\n",
        "        up = seed[seed >= 0].sum()/period\n",
        "        down = -seed[seed < 0].sum()/period\n",
        "        rs = up/down\n",
        "        rsi = np.zeros_like(prices)\n",
        "        rsi[:period] = 100. - 100./(1. + rs)\n",
        "\n",
        "        for i in range(period, len(prices)):\n",
        "            delta = deltas[i - 1]  # The difference between prices[i] and prices[i-1]\n",
        "            if delta > 0:\n",
        "                upval = delta\n",
        "                downval = 0.\n",
        "            else:\n",
        "                upval = 0.\n",
        "                downval = -delta\n",
        "\n",
        "            up = (up*(period-1) + upval)/period\n",
        "            down = (down*(period-1) + downval)/period\n",
        "\n",
        "            rs = up/down\n",
        "            rsi[i] = 100. - 100./(1. + rs)\n",
        "\n",
        "        return rsi\n",
        "\n",
        "    # Calculate EMAs, RSI, and find crossovers individually for each symbol\n",
        "    for symbol in final_df['Symbol'].unique():\n",
        "        symbol_df = final_df[final_df['Symbol'] == symbol].reset_index(drop=True)\n",
        "        prices = symbol_df['Close'].tolist()\n",
        "\n",
        "        # Check if there are enough data points to calculate both EMAs and RSI\n",
        "        if len(prices) < 50:\n",
        "            insufficient_data.append(symbol)\n",
        "            continue\n",
        "\n",
        "        ema20 = calculate_ema(prices, 20)\n",
        "        ema50 = calculate_ema(prices, 50)\n",
        "        rsi = calculate_rsi(prices)\n",
        "\n",
        "        for i in range(1, len(symbol_df)):\n",
        "            if not np.isnan(ema50[i]) and not np.isnan(ema20[i]) and not np.isnan(ema50[i - 1]) and not np.isnan(ema20[i - 1]):\n",
        "                if ema50[i] > ema20[i] and ema50[i - 1] < ema20[i - 1]:\n",
        "                    cross_message = f\"{symbol} - {symbol_df['Date'][i]} - Bull Close: {symbol_df['Close'][i]} - RSI: {rsi[i]:.2f}\"\n",
        "                    results.append([symbol, symbol_df['Date'][i], cross_message])\n",
        "                elif ema50[i] < ema20[i] and ema50[i - 1] > ema20[i - 1]:\n",
        "                    cross_message = f\"{symbol} - {symbol_df['Date'][i]} - Bear Close: {symbol_df['Close'][i]} - RSI: {rsi[i]:.2f}\"\n",
        "                    results.append([symbol, symbol_df['Date'][i], cross_message])\n",
        "\n",
        "    # Convert the results list to a DataFrame\n",
        "    cross_column_filtered = pd.DataFrame(results, columns=['Symbol', 'Date', '50 vs 20 EMA Cross with RSI'])\n",
        "\n",
        "    # Filter the DataFrame to include only those with a '50 vs 20 EMA Cross' within the last 300 days\n",
        "    three_hundred_days_ago = datetime.today() - timedelta(days=300)\n",
        "    cross_column_filtered['Date'] = pd.to_datetime(cross_column_filtered['Date'])\n",
        "    filtered_df = cross_column_filtered[cross_column_filtered['Date'] >= three_hundred_days_ago]\n",
        "\n",
        "    # Group by date and count the number of symbols having crossovers\n",
        "    cross_counts = filtered_df.groupby('Date').count()\n",
        "\n",
        "    # Filter for dates with crossovers of multiple symbols\n",
        "    multiple_cross_dates = cross_counts[cross_counts['Symbol'] > 1].index\n",
        "    multiple_cross_df = filtered_df[filtered_df['Date'].isin(multiple_cross_dates)]\n",
        "\n",
        "    # Filter for dates with crossovers of single symbols\n",
        "    single_cross_dates = cross_counts[cross_counts['Symbol'] == 1].index\n",
        "    single_cross_df = filtered_df[filtered_df['Date'].isin(single_cross_dates)]\n",
        "\n",
        "    # Sort both DataFrames by date\n",
        "    multiple_cross_df = multiple_cross_df.sort_values(by='Date', ascending=False)\n",
        "    single_cross_df = single_cross_df.sort_values(by='Date', ascending=False)\n",
        "\n",
        "    # Save the filtered DataFrames to CSV files\n",
        "    output_multiple_filtered_file_csv = 'ema_cross_results_multiple_symbols_with_rsi.csv'\n",
        "    multiple_cross_df.to_csv(output_multiple_filtered_file_csv, index=False)\n",
        "\n",
        "    output_single_filtered_file_csv = 'ema_cross_results_single_symbols_with_rsi.csv'\n",
        "    single_cross_df.to_csv(output_single_filtered_file_csv, index=False)\n",
        "\n",
        "    # Print the symbols with insufficient data for EMA calculation\n",
        "    # if insufficient_data:\n",
        "    #     print(\"Symbols with insufficient data for EMA calculation:\")\n",
        "    #     for symbol in insufficient_data:\n",
        "    #         print(symbol)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R4yD-geZbREC"
      },
      "outputs": [],
      "source": [
        "# Concatenate the results of multiple and single crossovers\n",
        "combined_df = pd.concat([multiple_cross_df, single_cross_df])\n",
        "\n",
        "# Drop duplicates if there are any\n",
        "combined_df = combined_df.drop_duplicates()\n",
        "\n",
        "# Sort the combined DataFrame by date in descending order\n",
        "combined_df = combined_df.sort_values(by='Date', ascending=False)\n",
        "\n",
        "# Display the combined DataFrame\n",
        "print(\"Combined dates with EMA crossovers:\")\n",
        "print(combined_df[['Symbol', '50 vs 20 EMA Cross with RSI']])\n",
        "\n",
        "# # Save the combined DataFrame to a CSV file\n",
        "# output_combined_file_csv = 'ema_cross_results_combined.csv'\n",
        "# combined_df.to_csv(output_combined_file_csv, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OzR00wQA0Xfh"
      },
      "outputs": [],
      "source": [
        "combined_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z5LjWlcGCBWl"
      },
      "outputs": [],
      "source": [
        "# combined_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from git import Repo\n",
        "\n",
        "# Use the GITHUB_WORKSPACE environment variable to get the path of the cloned repository\n",
        "repo_dir = os.environ.get('GITHUB_WORKSPACE', '/home/username/nepse_new')\n",
        "\n",
        "# Check if we're running in GitHub Actions\n",
        "if 'GITHUB_ACTIONS' in os.environ:\n",
        "    # We're in GitHub Actions, the repository is already cloned\n",
        "    repo = Repo(repo_dir)\n",
        "else:\n",
        "    # We're running locally, clone the repository\n",
        "    repo_url = \"https://github.com/iamsrijit/Nepse.git\"\n",
        "    try:\n",
        "        repo = Repo.clone_from(repo_url, repo_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Error cloning repository: {e}\")\n",
        "\n",
        "# Rest of your notebook code..."
      ],
      "metadata": {
        "id": "nISd06omREIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from git import Repo\n",
        "\n",
        "repo = Repo(os.environ.get('GITHUB_WORKSPACE', '/home/username/nepse_new'))\n",
        "\n",
        "# Make some changes to the repository...\n",
        "\n",
        "# Commit and push changes\n",
        "repo.git.add(A=True)\n",
        "repo.git.commit(m=\"Automated update\")\n",
        "\n",
        "# Use the GITHUB_TOKEN for authentication when pushing\n",
        "if 'GITHUB_TOKEN' in os.environ:\n",
        "    with repo.git.custom_environment(GIT_SSH_COMMAND='ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no'):\n",
        "        repo.git.push(f\"https://x-access-token:{os.environ['GITHUB_TOKEN']}@github.com/iamsrijit/Nepse.git\")\n",
        "else:\n",
        "    repo.git.push()"
      ],
      "metadata": {
        "id": "p2lssH5RRJws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N6R8PiMMIDPG",
        "outputId": "e832d217-02b9-4770-acbb-1e1c774f2a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to upload EMA_Cross for_2024-09-13.csv. Status code: 201\n",
            "Response Content: {\"content\":{\"name\":\"EMA_Cross for_2024-09-13.csv\",\"path\":\"EMA_Cross for_2024-09-13.csv\",\"sha\":\"2d76aede515eeaf6913fc68b614f701fe6827d0e\",\"size\":59446,\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/contents/EMA_Cross%20for_2024-09-13.csv?ref=main\",\"html_url\":\"https://github.com/iamsrijit/Nepse/blob/main/EMA_Cross%20for_2024-09-13.csv\",\"git_url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/blobs/2d76aede515eeaf6913fc68b614f701fe6827d0e\",\"download_url\":\"https://raw.githubusercontent.com/iamsrijit/Nepse/main/EMA_Cross%20for_2024-09-13.csv\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/iamsrijit/Nepse/contents/EMA_Cross%20for_2024-09-13.csv?ref=main\",\"git\":\"https://api.github.com/repos/iamsrijit/Nepse/git/blobs/2d76aede515eeaf6913fc68b614f701fe6827d0e\",\"html\":\"https://github.com/iamsrijit/Nepse/blob/main/EMA_Cross%20for_2024-09-13.csv\"}},\"commit\":{\"sha\":\"dbb33e49d27da5ae23e636602f2d2d425a474b9e\",\"node_id\":\"C_kwDOMHuaNtoAKGRiYjMzZTQ5ZDI3ZGE1YWUyM2U2MzY2MDJmMmQyZDQyNWE0NzRiOWU\",\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/commits/dbb33e49d27da5ae23e636602f2d2d425a474b9e\",\"html_url\":\"https://github.com/iamsrijit/Nepse/commit/dbb33e49d27da5ae23e636602f2d2d425a474b9e\",\"author\":{\"name\":\"srijit pokharel\",\"email\":\"46723334+iamsrijit@users.noreply.github.com\",\"date\":\"2024-09-13T09:28:46Z\"},\"committer\":{\"name\":\"srijit pokharel\",\"email\":\"46723334+iamsrijit@users.noreply.github.com\",\"date\":\"2024-09-13T09:28:46Z\"},\"tree\":{\"sha\":\"a7511a3e3e8246f943f2c6c658e03d7da7568f64\",\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/trees/a7511a3e3e8246f943f2c6c658e03d7da7568f64\"},\"message\":\"Upload EMA_Cross for_2024-09-13.csv\",\"parents\":[{\"sha\":\"ad4d699ac9ffd72efd2352ed17082625783566fb\",\"url\":\"https://api.github.com/repos/iamsrijit/Nepse/git/commits/ad4d699ac9ffd72efd2352ed17082625783566fb\",\"html_url\":\"https://github.com/iamsrijit/Nepse/commit/ad4d699ac9ffd72efd2352ed17082625783566fb\"}],\"verification\":{\"verified\":false,\"reason\":\"unsigned\",\"signature\":null,\"payload\":null}}}\n"
          ]
        }
      ],
      "source": [
        "# import requests\n",
        "# from datetime import datetime\n",
        "# import base64\n",
        "\n",
        "# # Assuming you have sorted_df DataFrame containing your data\n",
        "# # Assuming sorted_df is defined elsewhere in your code\n",
        "\n",
        "# try:\n",
        "#     # Convert sorted_df to CSV format\n",
        "#     csv_data = combined_df.to_csv(index=False)\n",
        "\n",
        "#     # Encode the CSV data to Base64\n",
        "#     csv_data_base64 = base64.b64encode(csv_data.encode()).decode()\n",
        "\n",
        "#     # Define the GitHub repository URL\n",
        "#     repo_url = 'https://github.com/iamsrijit/Nepse'\n",
        "\n",
        "#     # Define the file name with today's date\n",
        "#     file_name = f'EMA_Cross for_{datetime.today().strftime(\"%Y-%m-%d\")}.csv'\n",
        "\n",
        "#     # Define your personal access token\n",
        "#     token = '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'\n",
        "\n",
        "#     # Define the file path in the repository\n",
        "#     file_path = f'/{file_name}'\n",
        "\n",
        "#     # Define the API URL for uploading files to GitHub\n",
        "#     upload_url = f'https://api.github.com/repos/iamsrijit/Nepse/contents{file_path}'\n",
        "\n",
        "#     # Prepare the headers with the authorization token\n",
        "#     headers = {\n",
        "#         'Authorization': f'token {token}',\n",
        "#         'Accept': 'application/vnd.github.v3+json'\n",
        "#     }\n",
        "\n",
        "#     # Prepare the payload with file content\n",
        "#     payload = {\n",
        "#         'message': f'Upload {file_name}',\n",
        "#         'content': csv_data_base64,\n",
        "#         'branch': 'main'  # Specify the branch you want to upload to\n",
        "#     }\n",
        "\n",
        "#     # Send a PUT request to upload the file\n",
        "#     response = requests.put(upload_url, headers=headers, json=payload)\n",
        "\n",
        "#     # Check the response status\n",
        "#     if response.status_code == 200:\n",
        "#         print(f'File {file_name} uploaded successfully!')\n",
        "#     elif response.status_code == 422:\n",
        "#         print(f'Failed to upload {file_name}. Status code: 422 Unprocessable Entity')\n",
        "#         print('Error Message:', response.json()['message'])\n",
        "#     else:\n",
        "#         print(f'Failed to upload {file_name}. Status code: {response.status_code}')\n",
        "#         print('Response Content:', response.text)\n",
        "\n",
        "# except Exception as e:\n",
        "#     print('An error occurred:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONDn9XjDbFF1"
      },
      "source": [
        "**Delete from Github**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Seyhv-40MfwG"
      },
      "outputs": [],
      "source": [
        "from git import Repo\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Function to identify the latest file for a given pattern\n",
        "def get_latest_file(pattern):\n",
        "    files = [f for f in os.listdir(repo_dir) if re.match(pattern, f)]\n",
        "    if files:\n",
        "        latest_file = max(files)\n",
        "        return latest_file\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Replace with your GitHub repository URL\n",
        "repo_url = 'https://github.com/iamsrijit/Nepse.git'\n",
        "repo_dir = '/home/username/nepse_new'  # Update to a path with correct permissions\n",
        "\n",
        "# Check if the directory exists and is not empty\n",
        "if os.path.exists(repo_dir) and os.listdir(repo_dir):\n",
        "    repo = Repo(repo_dir)\n",
        "else:\n",
        "    # Clone the repository\n",
        "    try:\n",
        "        repo = Repo.clone_from(repo_url, repo_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Error cloning repository: {e}\")\n",
        "        raise\n",
        "\n",
        "# Delete older files for EMA_Cross for_\n",
        "latest_ema_file = get_latest_file('^EMA_Cross for_')\n",
        "if latest_ema_file:\n",
        "    print(f\"Latest EMA file: {latest_ema_file}\")\n",
        "    files_to_delete = [f for f in os.listdir(repo_dir) if re.match('^EMA_Cross for_', f) and f != latest_ema_file]\n",
        "    print(f\"Files to delete for EMA: {files_to_delete}\")\n",
        "    for file_to_delete in files_to_delete:\n",
        "        file_path = os.path.join(repo_dir, file_to_delete)\n",
        "        print(f\"Deleting file: {file_path}\")\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "            repo.index.remove([file_path], working_tree=True)\n",
        "            repo.index.commit(f'Deleted {file_to_delete}')\n",
        "            print(f\"Deleted {file_to_delete}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting {file_to_delete}: {e}\")\n",
        "\n",
        "# Delete older files for espen_\n",
        "latest_espen_file = get_latest_file('^espen_')\n",
        "if latest_espen_file:\n",
        "    print(f\"Latest espn file: {latest_espen_file}\")\n",
        "    files_to_delete = [f for f in os.listdir(repo_dir) if re.match('^espen_', f) and f != latest_espen_file]\n",
        "    print(f\"Files to delete for espn: {files_to_delete}\")\n",
        "    for file_to_delete in files_to_delete:\n",
        "        file_path = os.path.join(repo_dir, file_to_delete)\n",
        "        print(f\"Deleting file: {file_path}\")\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "            repo.index.remove([file_path], working_tree=True)\n",
        "            repo.index.commit(f'Deleted {file_to_delete}')\n",
        "            print(f\"Deleted {file_to_delete}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error deleting {file_to_delete}: {e}\")\n",
        "\n",
        "# Set up the remote repository with authentication\n",
        "origin = repo.remote(name='origin')\n",
        "origin.set_url('***github.com/iamsrijit/Nepse.git')\n",
        "\n",
        "# Push the changes to GitHub\n",
        "try:\n",
        "    origin.push()\n",
        "    print(\"Pushed changes to GitHub.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error pushing changes to GitHub: {e}\")\n",
        "\n",
        "print(\"Previous files have been deleted from GitHub repository.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cBAOm7ZLTa1p"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}